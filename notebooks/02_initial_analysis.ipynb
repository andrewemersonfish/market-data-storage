{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import os\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/andrewfish/Developer/Financial Data\")\n",
    "\n",
    "def download_opra_files(dates):\n",
    "    \"\"\"\n",
    "    Consolidates the download logic into one function that downloads\n",
    "    three types of gzipped CSV files (trades, minute_aggs, day_aggs)\n",
    "    for each date in the provided list of dates.\n",
    "    Returns a list of all successfully downloaded file paths.\n",
    "    \"\"\"\n",
    "    def download_b2_file(file_path: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Helper function describing how the file will be downloaded\n",
    "        from Backblaze using the new endpoint with a destination_dir.\n",
    "        \"\"\"\n",
    "        endpoint = \"http://localhost:8000/backblaze/download\"\n",
    "        \n",
    "        # Determine subdirectory based on file path\n",
    "        if \"trades_v1\" in file_path:\n",
    "            sub_dir = \"trades\"\n",
    "        elif \"minute_aggs_v1\" in file_path:\n",
    "            sub_dir = \"minute_aggs\"\n",
    "        elif \"day_aggs_v1\" in file_path:\n",
    "            sub_dir = \"day_aggs\"\n",
    "        else:\n",
    "            # If no recognized pattern, defaults to data/downloads\n",
    "            sub_dir = \"\"\n",
    "\n",
    "        destination_dir = os.path.join(\"data\", \"downloads\", sub_dir)\n",
    "        expected_path = os.path.join(destination_dir, os.path.basename(file_path))\n",
    "\n",
    "        try:\n",
    "            # Pass file_path and destination_dir as query params to the endpoint\n",
    "            response = requests.post(\n",
    "                endpoint, \n",
    "                params={\n",
    "                    \"file_path\": file_path,\n",
    "                    \"destination_dir\": destination_dir\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            return expected_path\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            #print(f\"Failed to download {file_path} - error: {e}\")\n",
    "            return None\n",
    "\n",
    "    downloaded_files = []\n",
    "    failed_files = []\n",
    "\n",
    "    # Collect all file paths for each date\n",
    "    for date in dates:\n",
    "        paths_to_download = [\n",
    "            f\"us_options_opra/trades_v1/{date.strftime('%Y')}/{date.strftime('%m')}/{date.strftime('%Y-%m-%d')}.csv.gz\",\n",
    "            f\"us_options_opra/minute_aggs_v1/{date.strftime('%Y')}/{date.strftime('%m')}/{date.strftime('%Y-%m-%d')}.csv.gz\",\n",
    "            f\"us_options_opra/day_aggs_v1/{date.strftime('%Y')}/{date.strftime('%m')}/{date.strftime('%Y-%m-%d')}.csv.gz\"\n",
    "        ]\n",
    "\n",
    "        for file_path in paths_to_download:\n",
    "            result = download_b2_file(file_path)\n",
    "            if result:\n",
    "                downloaded_files.append(result)\n",
    "            else:\n",
    "                failed_files.append(file_path)\n",
    "\n",
    "    # Report summary\n",
    "    print(\"\\nDownload Summary:\")\n",
    "    print(f\"Successfully downloaded: {len(downloaded_files)} files\")\n",
    "    print(f\"Failed downloads: {len(failed_files)} files\")\n",
    "\n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download Summary:\n",
      "Successfully downloaded: 372 files\n",
      "Failed downloads: 162 files\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dates to download\n",
    "start_date = datetime.datetime(2024, 6, 30)\n",
    "end_date = datetime.datetime(2024, 12, 24)\n",
    "\n",
    "dates_to_download = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    dates_to_download.append(current_date)\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "\n",
    "# Download the files\n",
    "downloaded_files = download_opra_files(dates_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily Aggregations DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28146631 entries, 0 to 28146630\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   ticker        object \n",
      " 1   volume        int64  \n",
      " 2   open          float64\n",
      " 3   close         float64\n",
      " 4   high          float64\n",
      " 5   low           float64\n",
      " 6   window_start  int64  \n",
      " 7   transactions  int64  \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 1.7+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get lists of files for each type\n",
    "#trade_files = glob.glob('data/downloads/trades/*.csv.gz')\n",
    "#minute_agg_files = glob.glob('data/downloads/minute_aggs/*.csv.gz')\n",
    "day_agg_files = glob.glob('data/downloads/day_aggs/*.csv.gz')\n",
    "\n",
    "# Create DataFrames for each type\n",
    "#trades_df = pd.concat([pd.read_csv(f) for f in trade_files], ignore_index=True)\n",
    "#minute_aggs_df = pd.concat([pd.read_csv(f) for f in minute_agg_files], ignore_index=True)\n",
    "day_aggs_df = pd.concat([pd.read_csv(f) for f in day_agg_files], ignore_index=True)\n",
    "\n",
    "# Print basic information about each DataFrame\n",
    "#print(\"\\nTrades DataFrame:\")\n",
    "#print(trades_df.info())\n",
    "#print(\"\\nMinute Aggregations DataFrame:\")\n",
    "#print(minute_aggs_df.info())\n",
    "print(\"\\nDaily Aggregations DataFrame:\")\n",
    "print(day_aggs_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>window_start</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O:A241220C00110000</td>\n",
       "      <td>25</td>\n",
       "      <td>31.65</td>\n",
       "      <td>31.70</td>\n",
       "      <td>31.70</td>\n",
       "      <td>31.65</td>\n",
       "      <td>1733893200000000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O:A241220C00130000</td>\n",
       "      <td>833</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.86</td>\n",
       "      <td>11.90</td>\n",
       "      <td>1733893200000000000</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O:A241220C00135000</td>\n",
       "      <td>17</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.59</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.59</td>\n",
       "      <td>1733893200000000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O:A241220C00140000</td>\n",
       "      <td>19</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1733893200000000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O:A241220C00145000</td>\n",
       "      <td>16</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1733893200000000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ticker  volume   open  close   high    low  \\\n",
       "0  O:A241220C00110000      25  31.65  31.70  31.70  31.65   \n",
       "1  O:A241220C00130000     833  12.45  12.00  12.86  11.90   \n",
       "2  O:A241220C00135000      17   7.70   7.59   7.70   7.59   \n",
       "3  O:A241220C00140000      19   3.70   3.40   4.20   3.40   \n",
       "4  O:A241220C00145000      16   1.80   1.10   1.80   1.10   \n",
       "\n",
       "          window_start  transactions  \n",
       "0  1733893200000000000             3  \n",
       "1  1733893200000000000            81  \n",
       "2  1733893200000000000             7  \n",
       "3  1733893200000000000             7  \n",
       "4  1733893200000000000             8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_aggs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ticker  volume   open  close   high    low  \\\n",
      "0  O:A241220C00110000      25  31.65  31.70  31.70  31.65   \n",
      "1  O:A241220C00130000     833  12.45  12.00  12.86  11.90   \n",
      "2  O:A241220C00135000      17   7.70   7.59   7.70   7.59   \n",
      "3  O:A241220C00140000      19   3.70   3.40   4.20   3.40   \n",
      "4  O:A241220C00145000      16   1.80   1.10   1.80   1.10   \n",
      "\n",
      "          window_start  transactions underlying_ticker expiration_date  \n",
      "0  1733893200000000000             3                 A      2024-12-20  \n",
      "1  1733893200000000000            81                 A      2024-12-20  \n",
      "2  1733893200000000000             7                 A      2024-12-20  \n",
      "3  1733893200000000000             7                 A      2024-12-20  \n",
      "4  1733893200000000000             8                 A      2024-12-20  \n"
     ]
    }
   ],
   "source": [
    "# Fix the underlying_ticker extraction to only get the actual ticker part\n",
    "day_aggs_df['underlying_ticker'] = day_aggs_df['ticker'].str.extract(r'O:([A-Z]+\\d*?)(?:\\d{6}[CP])')[0]\n",
    "\n",
    "# Display the result\n",
    "print(day_aggs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ticker underlying_ticker expiration_date\n",
      "0  O:A241220C00110000                 A      2024-12-20\n",
      "1  O:A241220C00130000                 A      2024-12-20\n",
      "2  O:A241220C00135000                 A      2024-12-20\n",
      "3  O:A241220C00140000                 A      2024-12-20\n",
      "4  O:A241220C00145000                 A      2024-12-20\n",
      "\n",
      "Updated underlying tickers:\n",
      "               ticker underlying_ticker\n",
      "0  O:A241220C00110000           A241220\n",
      "1  O:A241220C00130000           A241220\n",
      "2  O:A241220C00135000           A241220\n",
      "3  O:A241220C00140000           A241220\n",
      "4  O:A241220C00145000           A241220\n"
     ]
    }
   ],
   "source": [
    "# Extract expiration date from the option symbols and convert to datetime\n",
    "day_aggs_df['expiration_date'] = (\n",
    "    day_aggs_df['ticker']\n",
    "    .str.extract(r'[A-Z]+\\d*(\\d{6})[CP]')[0]  # Modified pattern to handle numbers in ticker\n",
    "    .apply(lambda x: datetime.datetime.strptime('20' + x, '%Y%m%d'))\n",
    ")\n",
    "\n",
    "# Display the result to verify\n",
    "print(day_aggs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ticker  volume   open  close   high    low  \\\n",
      "0  O:A241220C00110000      25  31.65  31.70  31.70  31.65   \n",
      "1  O:A241220C00130000     833  12.45  12.00  12.86  11.90   \n",
      "2  O:A241220C00135000      17   7.70   7.59   7.70   7.59   \n",
      "3  O:A241220C00140000      19   3.70   3.40   4.20   3.40   \n",
      "4  O:A241220C00145000      16   1.80   1.10   1.80   1.10   \n",
      "\n",
      "          window_start  transactions underlying_ticker expiration_date type  \n",
      "0  1733893200000000000             3                 A      2024-12-20    C  \n",
      "1  1733893200000000000            81                 A      2024-12-20    C  \n",
      "2  1733893200000000000             7                 A      2024-12-20    C  \n",
      "3  1733893200000000000             7                 A      2024-12-20    C  \n",
      "4  1733893200000000000             8                 A      2024-12-20    C  \n"
     ]
    }
   ],
   "source": [
    "# Extract contract type (P/C) from the option symbols\n",
    "day_aggs_df['type'] = day_aggs_df['ticker'].str.extract(r'(\\d{6})([CP])')[1]\n",
    "\n",
    "# Display the result\n",
    "print(day_aggs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ticker  volume   open  close   high    low  \\\n",
      "0  O:A241220C00110000      25  31.65  31.70  31.70  31.65   \n",
      "1  O:A241220C00130000     833  12.45  12.00  12.86  11.90   \n",
      "2  O:A241220C00135000      17   7.70   7.59   7.70   7.59   \n",
      "3  O:A241220C00140000      19   3.70   3.40   4.20   3.40   \n",
      "4  O:A241220C00145000      16   1.80   1.10   1.80   1.10   \n",
      "\n",
      "          window_start  transactions underlying_ticker expiration_date type  \\\n",
      "0  1733893200000000000             3                 A      2024-12-20    C   \n",
      "1  1733893200000000000            81                 A      2024-12-20    C   \n",
      "2  1733893200000000000             7                 A      2024-12-20    C   \n",
      "3  1733893200000000000             7                 A      2024-12-20    C   \n",
      "4  1733893200000000000             8                 A      2024-12-20    C   \n",
      "\n",
      "   strike  \n",
      "0   110.0  \n",
      "1   130.0  \n",
      "2   135.0  \n",
      "3   140.0  \n",
      "4   145.0  \n"
     ]
    }
   ],
   "source": [
    "# Extract strike price from the option symbols\n",
    "day_aggs_df['strike'] = (\n",
    "    day_aggs_df['ticker']\n",
    "    .str.extract(r'[CP](\\d+)$')[0]  # Extract all digits after P or C until the end\n",
    "    .astype(float) / 1000  # Convert to float and divide by 1000\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(day_aggs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to data/processed/day_aggs/processed_day_aggs.csv\n"
     ]
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('data/processed/day_aggs', exist_ok=True)\n",
    "\n",
    "# Save the processed DataFrame\n",
    "day_aggs_df.to_csv('data/processed/day_aggs/processed_day_aggs.csv', index=False)\n",
    "\n",
    "print(\"File saved successfully to data/processed/day_aggs/processed_day_aggs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique underlying tickers: 6057\n",
      "\n",
      "Top 20 tickers by total volume:\n",
      "underlying_ticker\n",
      "SPY     939036578\n",
      "QQQ     488488864\n",
      "NVDA    482102884\n",
      "SPXW    302572827\n",
      "TSLA    267744644\n",
      "IWM     190334838\n",
      "VIX     107789493\n",
      "AAPL    105612376\n",
      "AMD      73154011\n",
      "AMZN     70015318\n",
      "PLTR     66198497\n",
      "SPX      63333882\n",
      "TLT      61295329\n",
      "SMCI     54783477\n",
      "INTC     48330310\n",
      "META     42997284\n",
      "SLV      42460555\n",
      "FXI      39313660\n",
      "HYG      38670918\n",
      "MSTR     37861203\n",
      "Name: volume, dtype: int64\n",
      "\n",
      "Top 20 tickers by number of options:\n",
      "underlying_ticker\n",
      "SPXW    747081\n",
      "SPY     570526\n",
      "NVDA    440226\n",
      "QQQ     408337\n",
      "TSLA    285679\n",
      "SPX     266940\n",
      "IWM     251744\n",
      "MSTR    231374\n",
      "NDXP    217809\n",
      "META    210019\n",
      "SMCI    203790\n",
      "XSP     181165\n",
      "MSFT    158105\n",
      "AVGO    157600\n",
      "NFLX    144024\n",
      "COIN    140986\n",
      "TLT     140915\n",
      "AMD     139982\n",
      "RUTW    139154\n",
      "AAPL    132215\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique values for 'underlying_ticker'\n",
    "unique_underlying_tickers = day_aggs_df['underlying_ticker'].nunique()\n",
    "\n",
    "# Get the top tickers by volume\n",
    "top_tickers_by_volume = (\n",
    "    day_aggs_df.groupby('underlying_ticker')['volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)  # Show top 20 for now\n",
    ")\n",
    "\n",
    "# Get the top tickers by number of options\n",
    "top_tickers_by_options = (\n",
    "    day_aggs_df.groupby('underlying_ticker').size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)  # Show top 20 for now\n",
    ")\n",
    "\n",
    "print(f\"Number of unique underlying tickers: {unique_underlying_tickers}\")\n",
    "print(\"\\nTop 20 tickers by total volume:\")\n",
    "print(top_tickers_by_volume)\n",
    "print(\"\\nTop 20 tickers by number of options:\")\n",
    "print(top_tickers_by_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of unique tickers: 6057\n",
      "Filtered number of unique tickers: 6016\n",
      "\n",
      "Top 20 remaining tickers by volume:\n",
      "underlying_ticker\n",
      "NVDA     482102884\n",
      "TSLA     267744644\n",
      "AAPL     105612376\n",
      "AMD       73154011\n",
      "AMZN      70015318\n",
      "PLTR      66198497\n",
      "SMCI      54783477\n",
      "INTC      48330310\n",
      "META      42997284\n",
      "MSTR      37861203\n",
      "MARA      37765405\n",
      "MSFT      36138289\n",
      "SOFI      36085204\n",
      "GOOGL     33145800\n",
      "AVGO      30673879\n",
      "COIN      28109235\n",
      "GME       27686332\n",
      "BABA      25341547\n",
      "MU        25166866\n",
      "BITO      24810576\n",
      "Name: volume, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Common ETF and index prefixes/tickers to exclude\n",
    "etf_patterns = [\n",
    "    '^VIX', '^SPX', '^SPY', '^QQQ', '^IWM', '^DIA',  # Common indices\n",
    "    'VXX', 'UVXY', 'TVIX',  # Volatility ETFs\n",
    "    'SPY', 'QQQ', 'IWM', 'EEM', 'XLF', 'EFA', 'HYG', 'TLT',  # Popular ETFs\n",
    "    'VTI', 'VOO', 'VEA', 'BND', 'VWO', 'VUG', 'VTV',  # Vanguard ETFs\n",
    "    'DIA', 'XLE', 'XLK', 'XLV', 'XLI', 'XLP', 'XLU',  # Sector ETFs\n",
    "    'SLV', 'GLD', 'USO', 'UNG',  # Commodity ETFs\n",
    "     # Indices\n",
    "    'SPXW', 'SPX', 'VIX', 'NDX', 'RUT',\n",
    "    \n",
    "    # Leveraged ETFs\n",
    "    'TQQQ', 'SQQQ', 'UVXY', 'SOXL', 'SPXU', 'SPXS',\n",
    "    \n",
    "    # Country/Region ETFs\n",
    "    'FXI', 'EEM', 'EWZ', 'EFA', 'EWJ',\n",
    "    \n",
    "    # Sector/Asset ETFs\n",
    "    'XLE', 'XLF', 'XLK', 'XLI', 'XLP', 'GLD', 'SLV', 'USO',\n",
    "    \n",
    "    # Broad Market ETFs\n",
    "    'SPY', 'QQQ', 'IWM', 'DIA', 'VTI', 'VOO'\n",
    "]\n",
    "\n",
    "# Create a mask for non-ETF tickers\n",
    "non_etf_mask = ~day_aggs_df['underlying_ticker'].isin(etf_patterns)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = day_aggs_df[non_etf_mask].copy()\n",
    "\n",
    "# Show the comparison\n",
    "print(\"Original number of unique tickers:\", day_aggs_df['underlying_ticker'].nunique())\n",
    "print(\"Filtered number of unique tickers:\", filtered_df['underlying_ticker'].nunique())\n",
    "\n",
    "# Show top 20 remaining tickers by volume\n",
    "top_remaining = (\n",
    "    filtered_df.groupby('underlying_ticker')['volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)\n",
    ")\n",
    "print(\"\\nTop 20 remaining tickers by volume:\")\n",
    "print(top_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tickers in filtered dataset: 1000\n",
      "\n",
      "Top 20 tickers by volume:\n",
      "underlying_ticker\n",
      "NVDA     482102884\n",
      "TSLA     267744644\n",
      "AAPL     105612376\n",
      "AMD       73154011\n",
      "AMZN      70015318\n",
      "PLTR      66198497\n",
      "SMCI      54783477\n",
      "INTC      48330310\n",
      "META      42997284\n",
      "MSTR      37861203\n",
      "MARA      37765405\n",
      "MSFT      36138289\n",
      "SOFI      36085204\n",
      "GOOGL     33145800\n",
      "AVGO      30673879\n",
      "COIN      28109235\n",
      "GME       27686332\n",
      "BABA      25341547\n",
      "MU        25166866\n",
      "BITO      24810576\n",
      "Name: volume, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the top 1000 tickers by total options volume\n",
    "top_1000_tickers = (\n",
    "    filtered_df.groupby('underlying_ticker')['volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(1000)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to only include these tickers\n",
    "filtered_df = filtered_df[filtered_df['underlying_ticker'].isin(top_1000_tickers)].copy()\n",
    "\n",
    "# Print some statistics about the filtered dataset\n",
    "print(f\"Number of unique tickers in filtered dataset: {filtered_df['underlying_ticker'].nunique()}\")\n",
    "print(\"\\nTop 20 tickers by volume:\")\n",
    "print(\n",
    "    filtered_df.groupby('underlying_ticker')['volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in base DataFrame: 183000\n",
      "Number of unique dates: 183\n",
      "Number of unique tickers: 1000\n",
      "\n",
      "First few rows:\n",
      "             date underlying_ticker\n",
      "180000 2024-07-01                AA\n",
      "180001 2024-07-01               AAL\n",
      "180002 2024-07-01              AAOI\n",
      "180003 2024-07-01               AAP\n",
      "180004 2024-07-01              AAPL\n"
     ]
    }
   ],
   "source": [
    "# Get unique dates and tickers\n",
    "all_dates = filtered_df['expiration_date'].unique()\n",
    "all_tickers = filtered_df['underlying_ticker'].unique()\n",
    "\n",
    "# Create a cross product of dates and tickers\n",
    "date_index = pd.MultiIndex.from_product(\n",
    "    [all_dates, all_tickers],\n",
    "    names=['date', 'underlying_ticker']\n",
    ")\n",
    "\n",
    "# Create the base DataFrame with one row per date per ticker\n",
    "base_df = pd.DataFrame(index=date_index).reset_index()\n",
    "\n",
    "# Sort the DataFrame by date and ticker\n",
    "base_df = base_df.sort_values(['date', 'underlying_ticker'])\n",
    "\n",
    "# Save this base dataset\n",
    "base_df.to_csv('data/processed/day_aggs/base_df.csv', index=False)\n",
    "\n",
    "# Display some information about the new DataFrame\n",
    "print(f\"Total rows in base DataFrame: {len(base_df)}\")\n",
    "print(f\"Number of unique dates: {len(all_dates)}\")\n",
    "print(f\"Number of unique tickers: {len(all_tickers)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(base_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Convert dates to datetime if they aren't already\n",
    "base_df['date'] = pd.to_datetime(base_df['date'])\n",
    "\n",
    "# Get the overall date range\n",
    "start_date = base_df['date'].min()\n",
    "end_date = base_df['date'].max()\n",
    "\n",
    "# Function to fetch data for a single ticker\n",
    "def fetch_ticker_data(ticker):\n",
    "    try:\n",
    "        # Fetch data with yfinance\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(start=start_date, end=end_date + timedelta(days=1))\n",
    "        \n",
    "        # Reset index to make date a column and rename columns\n",
    "        hist = hist.reset_index()\n",
    "        hist.columns = hist.columns.str.lower()\n",
    "        \n",
    "        # Rename 'date' column if it's named differently\n",
    "        if 'date' not in hist.columns:\n",
    "            hist = hist.rename(columns={'index': 'date'})\n",
    "        \n",
    "        # Add ticker column\n",
    "        hist['underlying_ticker'] = ticker\n",
    "        \n",
    "        return hist[['date', 'underlying_ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create empty list to store DataFrames\n",
    "all_data = []\n",
    "\n",
    "# Process tickers in batches to show progress\n",
    "total_tickers = len(all_tickers)\n",
    "for i, ticker in enumerate(all_tickers, 1):\n",
    "    if i % 10 == 0:  # Print progress every 10 tickers\n",
    "        print(f\"Processing ticker {i} of {total_tickers}: {ticker}\")\n",
    "    \n",
    "    ticker_data = fetch_ticker_data(ticker)\n",
    "    if ticker_data is not None:\n",
    "        all_data.append(ticker_data)\n",
    "\n",
    "# Combine all data\n",
    "if all_data:\n",
    "    market_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Merge with base_df\n",
    "    result_df = pd.merge(\n",
    "        base_df,\n",
    "        market_data,\n",
    "        on=['date', 'underlying_ticker'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Save the result\n",
    "    result_df.to_csv('data/processed/day_aggs/base_df_with_market_data.csv', index=False)\n",
    "    \n",
    "    # Display some information about the result\n",
    "    print(\"\\nData fetching complete!\")\n",
    "    print(f\"Total rows in final DataFrame: {len(result_df)}\")\n",
    "    print(\"\\nSample of the data:\")\n",
    "    print(result_df.head())\n",
    "    \n",
    "    # Check for any missing data\n",
    "    missing_data = result_df[result_df['close'].isna()]\n",
    "    if len(missing_data) > 0:\n",
    "        print(f\"\\nNumber of rows with missing data: {len(missing_data)}\")\n",
    "        print(\"Sample of tickers with missing data:\")\n",
    "        print(missing_data['underlying_ticker'].value_counts().head())\n",
    "else:\n",
    "    print(\"No data was successfully fetched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data fetching complete!\n",
      "Total rows in final DataFrame: 183000\n",
      "\n",
      "Sample of the data:\n",
      "        date underlying_ticker        open        high         low  \\\n",
      "0 2024-07-01                AA   40.021413   40.637890   39.315446   \n",
      "1 2024-07-01               AAL   11.330000   11.410000   11.000000   \n",
      "2 2024-07-01              AAOI    8.390000    8.660000    7.850000   \n",
      "3 2024-07-01               AAP   61.909940   62.147368   58.813453   \n",
      "4 2024-07-01              AAPL  211.611974  217.019756  211.442359   \n",
      "\n",
      "        close      volume  \n",
      "0   39.474537   3228000.0  \n",
      "1   11.040000  28624300.0  \n",
      "2    8.080000   1778500.0  \n",
      "3   59.070667   1767600.0  \n",
      "4  216.261475  60402900.0  \n",
      "\n",
      "Number of rows with missing data: 74095\n",
      "Sample of tickers with missing data:\n",
      "underlying_ticker\n",
      "BABA1    183\n",
      "WHR      183\n",
      "WYNN     183\n",
      "WW       183\n",
      "WULF     183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert market_data dates to naive datetime (remove timezone)\n",
    "market_data['date'] = market_data['date'].dt.tz_localize(None)\n",
    "\n",
    "# Now try the merge again\n",
    "result_df = pd.merge(\n",
    "    base_df,\n",
    "    market_data,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the result\n",
    "result_df.to_csv('data/processed/day_aggs/base_df_with_market_data.csv', index=False)\n",
    "\n",
    "# Display some information about the result\n",
    "print(\"\\nData fetching complete!\")\n",
    "print(f\"Total rows in final DataFrame: {len(result_df)}\")\n",
    "print(\"\\nSample of the data:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Check for any missing data\n",
    "missing_data = result_df[result_df['close'].isna()]\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"\\nNumber of rows with missing data: {len(missing_data)}\")\n",
    "    print(\"Sample of tickers with missing data:\")\n",
    "    print(missing_data['underlying_ticker'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data percentage: 40.49%\n",
      "\n",
      "Dates with most missing data:\n",
      "date\n",
      "2024-07-01    131\n",
      "2024-07-02    131\n",
      "2024-07-03    131\n",
      "2024-07-05    131\n",
      "2024-07-08    131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tickers with completely missing data:\n",
      "underlying_ticker\n",
      "BABA1    183\n",
      "BIG      183\n",
      "BRKB     183\n",
      "DJX      183\n",
      "GPS      183\n",
      "        ... \n",
      "ZIM      183\n",
      "ZION     183\n",
      "ZM       183\n",
      "ZS       183\n",
      "ZTO      183\n",
      "Length: 127, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(completely_missing)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Remove weekends and holidays (optional)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m is_trading_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m~\u001b[39;49m\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m trading_days \u001b[38;5;241m=\u001b[39m is_trading_day[is_trading_day]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Filter for only trading days\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/core/generic.py:1571\u001b[0m, in \u001b[0;36mNDFrame.__invert__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;66;03m# inv fails with 0 len\u001b[39;00m\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1571\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__invert__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:361\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[0;32m--> 361\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# First, let's see what percentage of our data is missing\n",
    "total_rows = len(result_df)\n",
    "missing_rows = len(missing_data)\n",
    "print(f\"Missing data percentage: {(missing_rows/total_rows)*100:.2f}%\")\n",
    "\n",
    "# Check if missing data is concentrated on specific dates (like weekends)\n",
    "missing_by_date = missing_data['date'].value_counts().sort_index()\n",
    "print(\"\\nDates with most missing data:\")\n",
    "print(missing_by_date.head())\n",
    "\n",
    "# Check tickers with 100% missing data\n",
    "completely_missing = missing_data.groupby('underlying_ticker').size()\n",
    "completely_missing = completely_missing[completely_missing == 183]  # 183 is the number of dates per ticker\n",
    "print(\"\\nTickers with completely missing data:\")\n",
    "print(completely_missing)\n",
    "\n",
    "# Remove weekends and holidays (optional)\n",
    "is_trading_day = ~result_df['close'].isna().groupby(result_df['date']).mean().round()\n",
    "trading_days = is_trading_day[is_trading_day].index\n",
    "\n",
    "# Filter for only trading days\n",
    "clean_df = result_df[result_df['date'].isin(trading_days)].copy()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned dataset summary:\")\n",
    "print(f\"Original rows: {len(result_df)}\")\n",
    "print(f\"Cleaned rows: {len(clean_df)}\")\n",
    "print(f\"Missing data in cleaned dataset: {clean_df['close'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get list of tickers with completely missing data\n",
    "missing_tickers = completely_missing.index.tolist()\n",
    "\n",
    "# Function to fetch data for a single ticker (with delay)\n",
    "def fetch_ticker_data(ticker):\n",
    "    try:\n",
    "        # Add small delay between requests\n",
    "        time.sleep(0.5)  # 500ms delay\n",
    "        \n",
    "        # Try alternative symbols for some known cases\n",
    "        if ticker == 'BRKB':\n",
    "            ticker = 'BRK-B'\n",
    "        \n",
    "        # Fetch data with yfinance\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(start=start_date, end=end_date + timedelta(days=1))\n",
    "        \n",
    "        # Reset index to make date a column and rename columns\n",
    "        hist = hist.reset_index()\n",
    "        hist.columns = hist.columns.str.lower()\n",
    "        \n",
    "        # Rename 'date' column if it's named differently\n",
    "        if 'date' not in hist.columns:\n",
    "            hist = hist.rename(columns={'index': 'date'})\n",
    "        \n",
    "        # Add ticker column (use original ticker name)\n",
    "        hist['underlying_ticker'] = ticker\n",
    "        \n",
    "        return hist[['date', 'underlying_ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create empty list to store new data\n",
    "new_data = []\n",
    "\n",
    "# Process missing tickers\n",
    "total_missing = len(missing_tickers)\n",
    "for i, ticker in enumerate(missing_tickers, 1):\n",
    "    print(f\"Processing missing ticker {i} of {total_missing}: {ticker}\")\n",
    "    ticker_data = fetch_ticker_data(ticker)\n",
    "    if ticker_data is not None:\n",
    "        new_data.append(ticker_data)\n",
    "\n",
    "# If we got any new data, add it to our result_df\n",
    "if new_data:\n",
    "    # Combine new data\n",
    "    new_market_data = pd.concat(new_data, ignore_index=True)\n",
    "    \n",
    "    # Remove timezone info\n",
    "    new_market_data['date'] = new_market_data['date'].dt.tz_localize(None)\n",
    "    \n",
    "    # Remove old data for these tickers from result_df\n",
    "    result_df = result_df[~result_df['underlying_ticker'].isin(missing_tickers)]\n",
    "    \n",
    "    # Add new data\n",
    "    new_result_df = pd.merge(\n",
    "        base_df[base_df['underlying_ticker'].isin(missing_tickers)],\n",
    "        new_market_data,\n",
    "        on=['date', 'underlying_ticker'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Combine old and new data\n",
    "    result_df = pd.concat([result_df, new_result_df], ignore_index=True)\n",
    "    \n",
    "    # Sort the DataFrame\n",
    "    result_df = result_df.sort_values(['date', 'underlying_ticker'])\n",
    "    \n",
    "    # Save updated dataset\n",
    "    result_df.to_csv('data/processed/day_aggs/base_df_with_market_data.csv', index=False)\n",
    "    \n",
    "    # Show updated statistics\n",
    "    missing_data = result_df[result_df['close'].isna()]\n",
    "    print(\"\\nUpdated statistics:\")\n",
    "    print(f\"Total rows: {len(result_df)}\")\n",
    "    print(f\"Missing data rows: {len(missing_data)}\")\n",
    "    print(f\"Missing data percentage: {(len(missing_data)/len(result_df))*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No new data was successfully fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned dataset summary:\n",
      "Original rows: 183000\n",
      "Cleaned rows: 125000\n",
      "Missing data in cleaned dataset: 3220\n",
      "\n",
      "Number of records per date in cleaned dataset:\n",
      "date\n",
      "2024-07-01    972\n",
      "2024-07-02    972\n",
      "2024-07-03    972\n",
      "2024-07-05    972\n",
      "2024-07-08    972\n",
      "Name: close, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of non-null values for each date\n",
    "trading_day_counts = result_df.groupby('date')['close'].count()\n",
    "total_tickers = result_df['underlying_ticker'].nunique()\n",
    "trading_day_pct = trading_day_counts / total_tickers\n",
    "\n",
    "# Consider a day a trading day if it has data for at least 50% of tickers\n",
    "trading_days = trading_day_pct[trading_day_pct > 0.5].index\n",
    "\n",
    "# Filter for only trading days\n",
    "clean_df = result_df[result_df['date'].isin(trading_days)].copy()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned dataset summary:\")\n",
    "print(f\"Original rows: {len(result_df)}\")\n",
    "print(f\"Cleaned rows: {len(clean_df)}\")\n",
    "print(f\"Missing data in cleaned dataset: {clean_df['close'].isna().sum()}\")\n",
    "\n",
    "# Show distribution of data by date\n",
    "print(\"\\nNumber of records per date in cleaned dataset:\")\n",
    "print(clean_df.groupby('date')['close'].count().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>underlying_ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>40.021413</td>\n",
       "      <td>40.637890</td>\n",
       "      <td>39.315446</td>\n",
       "      <td>39.474537</td>\n",
       "      <td>3228000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAL</td>\n",
       "      <td>11.330000</td>\n",
       "      <td>11.410000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.040000</td>\n",
       "      <td>28624300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>8.390000</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>8.080000</td>\n",
       "      <td>1778500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAP</td>\n",
       "      <td>61.909940</td>\n",
       "      <td>62.147368</td>\n",
       "      <td>58.813453</td>\n",
       "      <td>59.070667</td>\n",
       "      <td>1767600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>211.611974</td>\n",
       "      <td>217.019756</td>\n",
       "      <td>211.442359</td>\n",
       "      <td>216.261475</td>\n",
       "      <td>60402900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date underlying_ticker        open        high         low  \\\n",
       "0 2024-07-01                AA   40.021413   40.637890   39.315446   \n",
       "1 2024-07-01               AAL   11.330000   11.410000   11.000000   \n",
       "2 2024-07-01              AAOI    8.390000    8.660000    7.850000   \n",
       "3 2024-07-01               AAP   61.909940   62.147368   58.813453   \n",
       "4 2024-07-01              AAPL  211.611974  217.019756  211.442359   \n",
       "\n",
       "        close      volume  \n",
       "0   39.474537   3228000.0  \n",
       "1   11.040000  28624300.0  \n",
       "2    8.080000   1778500.0  \n",
       "3   59.070667   1767600.0  \n",
       "4  216.261475  60402900.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with option volumes:\n",
      "        date underlying_ticker       close      volume  total_option_volume\n",
      "0 2024-07-01                AA   39.474537   3228000.0              18243.0\n",
      "1 2024-07-01               AAL   11.040000  28624300.0             103084.0\n",
      "2 2024-07-01              AAOI    8.080000   1778500.0               2974.0\n",
      "3 2024-07-01               AAP   59.070667   1767600.0               6723.0\n",
      "4 2024-07-01              AAPL  216.261475  60402900.0            1432971.0\n",
      "\n",
      "Top 10 days by option volume:\n",
      "date\n",
      "2024-12-20    40736562.0\n",
      "2024-11-08    40670366.0\n",
      "2024-11-07    39613245.0\n",
      "2024-11-15    39067922.0\n",
      "2024-11-06    38337638.0\n",
      "2024-11-22    37215509.0\n",
      "2024-12-06    37154362.0\n",
      "2024-11-11    37138074.0\n",
      "2024-12-18    36554656.0\n",
      "2024-12-13    36424182.0\n",
      "Name: total_option_volume, dtype: float64\n",
      "\n",
      "Top 10 tickers by total option volume:\n",
      "underlying_ticker\n",
      "NVDA    482102884.0\n",
      "TSLA    267744644.0\n",
      "AAPL    105612376.0\n",
      "AMD      73154011.0\n",
      "AMZN     70015318.0\n",
      "PLTR     66198497.0\n",
      "SMCI     54783477.0\n",
      "INTC     48330310.0\n",
      "META     42997284.0\n",
      "MSTR     37861203.0\n",
      "Name: total_option_volume, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert window_start from nanoseconds to datetime and normalize to remove time\n",
    "filtered_df['trading_date'] = pd.to_datetime(filtered_df['window_start'] / 1e9, unit='s').dt.normalize()\n",
    "\n",
    "# Calculate daily option volume\n",
    "daily_option_volume = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])['volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'trading_date': 'date',\n",
    "        'volume': 'total_option_volume'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Ensure clean_df dates are normalized\n",
    "clean_df['date'] = pd.to_datetime(clean_df['date']).dt.normalize()\n",
    "\n",
    "# Merge with clean_df (drop any existing option volume columns first)\n",
    "option_cols = [col for col in clean_df.columns if 'option_volume' in col]\n",
    "clean_df = clean_df.drop(columns=option_cols, errors='ignore')\n",
    "\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    daily_option_volume,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill any missing option volume with 0\n",
    "clean_df['total_option_volume'] = clean_df['total_option_volume'].fillna(0)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display some statistics\n",
    "print(\"Sample of data with option volumes:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'close', 'volume', 'total_option_volume']].head())\n",
    "\n",
    "print(\"\\nTop 10 days by option volume:\")\n",
    "print(\n",
    "    clean_df.groupby('date')['total_option_volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 tickers by total option volume:\")\n",
    "print(\n",
    "    clean_df.groupby('underlying_ticker')['total_option_volume']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/3503309255.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_hhi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with HHI:\n",
      "        date underlying_ticker  total_option_volume  option_hhi\n",
      "0 2024-07-01                AA              18243.0    0.044966\n",
      "1 2024-07-01               AAL             103084.0    0.042765\n",
      "2 2024-07-01              AAOI               2974.0    0.148861\n",
      "3 2024-07-01               AAP               6723.0    0.027098\n",
      "4 2024-07-01              AAPL            1432971.0    0.033944\n",
      "\n",
      "Top 10 most concentrated days (highest average HHI):\n",
      "date\n",
      "2024-09-10    0.143076\n",
      "2024-09-17    0.141824\n",
      "2024-10-10    0.141173\n",
      "2024-09-30    0.139684\n",
      "2024-08-16    0.139001\n",
      "2024-09-20    0.138560\n",
      "2024-10-01    0.138424\n",
      "2024-09-25    0.137635\n",
      "2024-09-05    0.136955\n",
      "2024-10-23    0.136635\n",
      "Name: option_hhi, dtype: float64\n",
      "\n",
      "Top 10 most concentrated tickers (highest average HHI):\n",
      "underlying_ticker\n",
      "MUB      0.760964\n",
      "AMCR     0.729887\n",
      "MULN1    0.717769\n",
      "EDR      0.649502\n",
      "ARHS     0.624717\n",
      "GSM      0.598321\n",
      "YMM      0.595236\n",
      "CRTO     0.589771\n",
      "INFN     0.585690\n",
      "BBD      0.585514\n",
      "Name: option_hhi, dtype: float64\n",
      "\n",
      "HHI Summary Statistics (for days with significant volume):\n",
      "count    96106.000000\n",
      "mean         0.126295\n",
      "std          0.141314\n",
      "min          0.003176\n",
      "25%          0.038722\n",
      "50%          0.076249\n",
      "75%          0.154759\n",
      "max          0.999896\n",
      "Name: option_hhi, dtype: float64\n",
      "\n",
      "Top 10 tickers by volume-weighted HHI:\n",
      "underlying_ticker\n",
      "NVDA    0.002146\n",
      "TSLA    0.001530\n",
      "AAPL    0.000977\n",
      "AMZN    0.000601\n",
      "AMD     0.000546\n",
      "BITO    0.000497\n",
      "SOFI    0.000404\n",
      "AAL     0.000397\n",
      "LQD     0.000389\n",
      "KWEB    0.000380\n",
      "Name: volume_weighted_hhi, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First, let's calculate the HHI for each ticker-day\n",
    "def calculate_hhi(group):\n",
    "    total_volume = group['volume'].sum()\n",
    "    if total_volume == 0:\n",
    "        return 0\n",
    "    market_shares = group['volume'] / total_volume\n",
    "    return (market_shares ** 2).sum()\n",
    "\n",
    "# Calculate HHI for each ticker-day\n",
    "hhi_by_day = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker', 'ticker'])['volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .groupby(['trading_date', 'underlying_ticker'])\n",
    "    .apply(calculate_hhi)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'option_hhi'})\n",
    ")\n",
    "\n",
    "# Normalize dates\n",
    "hhi_by_day['trading_date'] = pd.to_datetime(hhi_by_day['trading_date']).dt.normalize()\n",
    "hhi_by_day = hhi_by_day.rename(columns={'trading_date': 'date'})\n",
    "\n",
    "# Merge HHI with clean_df\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    hhi_by_day,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing HHI values with 0\n",
    "clean_df['option_hhi'] = clean_df['option_hhi'].fillna(0)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display some statistics\n",
    "print(\"Sample of data with HHI:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'total_option_volume', 'option_hhi']].head())\n",
    "\n",
    "print(\"\\nTop 10 most concentrated days (highest average HHI):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]  # Filter for meaningful volume\n",
    "    .groupby('date')['option_hhi']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 most concentrated tickers (highest average HHI):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]  # Filter for meaningful volume\n",
    "    .groupby('underlying_ticker')['option_hhi']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Calculate some summary statistics\n",
    "print(\"\\nHHI Summary Statistics (for days with significant volume):\")\n",
    "hhi_stats = clean_df[clean_df['total_option_volume'] > 1000]['option_hhi'].describe()\n",
    "print(hhi_stats)\n",
    "\n",
    "# Optional: Create volume-weighted HHI\n",
    "clean_df['volume_weighted_hhi'] = clean_df['option_hhi'] * (clean_df['total_option_volume'] / clean_df.groupby('date')['total_option_volume'].transform('sum'))\n",
    "\n",
    "print(\"\\nTop 10 tickers by volume-weighted HHI:\")\n",
    "print(\n",
    "    clean_df.groupby('underlying_ticker')['volume_weighted_hhi']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/646860361.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_weighted_strike_distance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with weighted strike distances:\n",
      "        date underlying_ticker       close  total_option_volume  \\\n",
      "0 2024-07-01                AA   39.474537              18243.0   \n",
      "1 2024-07-01               AAL   11.040000             103084.0   \n",
      "2 2024-07-01              AAOI    8.080000               2974.0   \n",
      "3 2024-07-01               AAP   59.070667               6723.0   \n",
      "4 2024-07-01              AAPL  216.261475            1432971.0   \n",
      "\n",
      "   weighted_strike_distance  \n",
      "0              48811.239887  \n",
      "1              15258.147886  \n",
      "2               4045.293617  \n",
      "3              64496.290477  \n",
      "4              71137.721162  \n",
      "\n",
      "Top 10 days by average absolute strike distance (minimum volume filter):\n",
      "date\n",
      "2024-07-26    126972.298115\n",
      "2024-12-13    125204.457336\n",
      "2024-09-27    122304.772522\n",
      "2024-08-09    121666.082618\n",
      "2024-11-29    120491.806344\n",
      "2024-08-30    118227.231546\n",
      "2024-07-12    115888.611273\n",
      "2024-07-05    115696.822217\n",
      "2024-09-13    115039.986952\n",
      "2024-11-01    114507.266995\n",
      "Name: weighted_strike_distance, dtype: float64\n",
      "\n",
      "Top 10 tickers by average absolute strike distance (minimum volume filter):\n",
      "underlying_ticker\n",
      "NDXP    1.274691e+07\n",
      "RUTW    1.758052e+06\n",
      "XSP     1.416187e+06\n",
      "BKNG    1.076365e+06\n",
      "COST    6.393311e+05\n",
      "NFLX    6.094289e+05\n",
      "LLY     5.197859e+05\n",
      "META    4.946665e+05\n",
      "MELI    3.899473e+05\n",
      "GS      3.411567e+05\n",
      "Name: weighted_strike_distance, dtype: float64\n",
      "\n",
      "Weighted Strike Distance Summary Statistics (for days with significant volume):\n",
      "count    9.610600e+04\n",
      "mean     6.209895e+04\n",
      "std      5.069367e+05\n",
      "min      4.704244e+01\n",
      "25%      5.479197e+03\n",
      "50%      1.372846e+04\n",
      "75%      3.707960e+04\n",
      "max      2.176622e+07\n",
      "Name: weighted_strike_distance, dtype: float64\n",
      "\n",
      "Separate Call/Put Distance Statistics:\n",
      "                        trading_date  call_distance  put_distance\n",
      "count                         122487   1.224870e+05  1.224870e+05\n",
      "mean   2024-09-26 20:44:59.845697792   5.688612e+04  4.614360e+04\n",
      "min              2024-07-01 00:00:00  -7.382199e+01 -4.425988e+00\n",
      "25%              2024-08-14 00:00:00   4.610368e+03  3.054253e+03\n",
      "50%              2024-09-27 00:00:00   1.104125e+04  8.507662e+03\n",
      "75%              2024-11-11 00:00:00   3.051151e+04  2.594715e+04\n",
      "max              2024-12-24 00:00:00   2.944359e+07  2.666927e+07\n",
      "std                              NaN   5.208244e+05  4.131775e+05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/646860361.py:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_weighted_strike_distance_by_type)\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage distance from current price for each contract\n",
    "def calculate_weighted_strike_distance(group):\n",
    "    if len(group) == 0 or group['volume'].sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate percentage distance from current price for each contract\n",
    "    pct_distances = (group['strike'] - group['close']) / group['close'] * 100\n",
    "    \n",
    "    # Calculate volume weights\n",
    "    weights = group['volume'] / group['volume'].sum()\n",
    "    \n",
    "    # Return weighted average distance\n",
    "    return (pct_distances * weights).sum()\n",
    "\n",
    "# Calculate weighted strike distance for each ticker-day\n",
    "strike_distances = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])\n",
    "    .apply(calculate_weighted_strike_distance)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'weighted_strike_distance'})\n",
    ")\n",
    "\n",
    "# Normalize dates\n",
    "strike_distances['trading_date'] = pd.to_datetime(strike_distances['trading_date']).dt.normalize()\n",
    "strike_distances = strike_distances.rename(columns={'trading_date': 'date'})\n",
    "\n",
    "# Merge with clean_df\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    strike_distances,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "clean_df['weighted_strike_distance'] = clean_df['weighted_strike_distance'].fillna(0)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Sample of data with weighted strike distances:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'close', 'total_option_volume', 'weighted_strike_distance']].head())\n",
    "\n",
    "print(\"\\nTop 10 days by average absolute strike distance (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('date')['weighted_strike_distance']\n",
    "    .mean()\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 tickers by average absolute strike distance (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('underlying_ticker')['weighted_strike_distance']\n",
    "    .mean()\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nWeighted Strike Distance Summary Statistics (for days with significant volume):\")\n",
    "distance_stats = clean_df[clean_df['total_option_volume'] > 1000]['weighted_strike_distance'].describe()\n",
    "print(distance_stats)\n",
    "\n",
    "# Optional: Calculate separate stats for calls and puts\n",
    "def calculate_weighted_strike_distance_by_type(group):\n",
    "    if len(group) == 0 or group['volume'].sum() == 0:\n",
    "        return pd.Series({'call_distance': 0, 'put_distance': 0})\n",
    "    \n",
    "    # Split into calls and puts\n",
    "    calls = group[group['type'] == 'C']\n",
    "    puts = group[group['type'] == 'P']\n",
    "    \n",
    "    # Calculate for calls\n",
    "    if len(calls) > 0 and calls['volume'].sum() > 0:\n",
    "        call_weights = calls['volume'] / calls['volume'].sum()\n",
    "        call_distance = ((calls['strike'] - calls['close']) / calls['close'] * 100 * call_weights).sum()\n",
    "    else:\n",
    "        call_distance = 0\n",
    "        \n",
    "    # Calculate for puts\n",
    "    if len(puts) > 0 and puts['volume'].sum() > 0:\n",
    "        put_weights = puts['volume'] / puts['volume'].sum()\n",
    "        put_distance = ((puts['strike'] - puts['close']) / puts['close'] * 100 * put_weights).sum()\n",
    "    else:\n",
    "        put_distance = 0\n",
    "    \n",
    "    return pd.Series({'call_distance': call_distance, 'put_distance': put_distance})\n",
    "\n",
    "# Calculate separate distances for calls and puts\n",
    "type_distances = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])\n",
    "    .apply(calculate_weighted_strike_distance_by_type)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nSeparate Call/Put Distance Statistics:\")\n",
    "print(type_distances.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in clean_df:\n",
      "['date', 'underlying_ticker', 'open', 'high', 'low', 'close', 'volume', 'total_option_volume', 'option_hhi', 'volume_weighted_hhi', 'weighted_strike_distance']\n",
      "\n",
      "Sample of clean_df with key metrics:\n",
      "        date underlying_ticker       close  total_option_volume  option_hhi  \\\n",
      "0 2024-07-01                AA   39.474537              18243.0    0.044966   \n",
      "1 2024-07-01               AAL   11.040000             103084.0    0.042765   \n",
      "2 2024-07-01              AAOI    8.080000               2974.0    0.148861   \n",
      "3 2024-07-01               AAP   59.070667               6723.0    0.027098   \n",
      "4 2024-07-01              AAPL  216.261475            1432971.0    0.033944   \n",
      "\n",
      "   weighted_strike_distance  \n",
      "0              48811.239887  \n",
      "1              15258.147886  \n",
      "2               4045.293617  \n",
      "3              64496.290477  \n",
      "4              71137.721162  \n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in clean_df:\")\n",
    "print(clean_df.columns.tolist())\n",
    "\n",
    "# Show a sample with our key metrics\n",
    "print(\"\\nSample of clean_df with key metrics:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'close', 'total_option_volume', 'option_hhi', 'weighted_strike_distance']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/4247650816.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_log_moneyness_skew)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with log-moneyness skew:\n",
      "        date underlying_ticker       close  total_option_volume  \\\n",
      "0 2024-07-01                AA   39.474537              18243.0   \n",
      "1 2024-07-01               AAL   11.040000             103084.0   \n",
      "2 2024-07-01              AAOI    8.080000               2974.0   \n",
      "3 2024-07-01               AAP   59.070667               6723.0   \n",
      "4 2024-07-01              AAPL  216.261475            1432971.0   \n",
      "\n",
      "   log_moneyness_skew  \n",
      "0           -0.685844  \n",
      "1           -0.305484  \n",
      "2            0.284867  \n",
      "3           -0.815526  \n",
      "4            1.070264  \n",
      "\n",
      "Top 10 days by average absolute log-moneyness skew (minimum volume filter):\n",
      "date\n",
      "2024-12-18    0.828945\n",
      "2024-12-19    0.811939\n",
      "2024-08-07    0.665583\n",
      "2024-09-06    0.637450\n",
      "2024-08-05    0.566172\n",
      "2024-12-23    0.485638\n",
      "2024-08-02    0.481456\n",
      "2024-09-05    0.480052\n",
      "2024-12-12    0.466207\n",
      "2024-07-18    0.449758\n",
      "Name: log_moneyness_skew, dtype: float64\n",
      "\n",
      "Top 10 tickers by average absolute log-moneyness skew (minimum volume filter):\n",
      "underlying_ticker\n",
      "MULN1    3.891981\n",
      "NKLA1    3.603354\n",
      "SIRI1    2.451389\n",
      "SQQQ1    2.023452\n",
      "DLTR     1.977757\n",
      "MAXN     1.863906\n",
      "MATV     1.751491\n",
      "MRNA     1.606502\n",
      "OPTT     1.585641\n",
      "SPWR     1.491986\n",
      "Name: log_moneyness_skew, dtype: float64\n",
      "\n",
      "Log-Moneyness Skew Summary Statistics (for days with significant volume):\n",
      "count    96106.000000\n",
      "mean        -0.126899\n",
      "std          1.013597\n",
      "min         -5.829109\n",
      "25%         -0.753667\n",
      "50%         -0.115129\n",
      "75%          0.513446\n",
      "max          6.804798\n",
      "Name: log_moneyness_skew, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_log_moneyness_skew(group):\n",
    "    if len(group) == 0 or group['volume'].sum() == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate log-moneyness for each contract\n",
    "    log_moneyness = np.log(group['strike'] / group['close'])\n",
    "    \n",
    "    # Split into calls and puts\n",
    "    calls = group[group['type'] == 'C']\n",
    "    puts = group[group['type'] == 'P']\n",
    "    \n",
    "    # Calculate weighted log-moneyness for calls\n",
    "    if len(calls) > 0 and calls['volume'].sum() > 0:\n",
    "        call_weights = calls['volume'] / calls['volume'].sum()\n",
    "        call_weighted_moneyness = (np.log(calls['strike'] / calls['close']) * call_weights).sum()\n",
    "    else:\n",
    "        call_weighted_moneyness = 0\n",
    "        \n",
    "    # Calculate weighted log-moneyness for puts\n",
    "    if len(puts) > 0 and puts['volume'].sum() > 0:\n",
    "        put_weights = puts['volume'] / puts['volume'].sum()\n",
    "        put_weighted_moneyness = (np.log(puts['strike'] / puts['close']) * put_weights).sum()\n",
    "    else:\n",
    "        put_weighted_moneyness = 0\n",
    "    \n",
    "    # Return the skew (put minus call)\n",
    "    return put_weighted_moneyness - call_weighted_moneyness\n",
    "\n",
    "# Calculate log-moneyness skew for each ticker-day\n",
    "moneyness_skew = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])\n",
    "    .apply(calculate_log_moneyness_skew)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'log_moneyness_skew'})\n",
    ")\n",
    "\n",
    "# Normalize dates\n",
    "moneyness_skew['trading_date'] = pd.to_datetime(moneyness_skew['trading_date']).dt.normalize()\n",
    "moneyness_skew = moneyness_skew.rename(columns={'trading_date': 'date'})\n",
    "\n",
    "# Merge with clean_df\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    moneyness_skew,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "clean_df['log_moneyness_skew'] = clean_df['log_moneyness_skew'].fillna(0)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Sample of data with log-moneyness skew:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'close', 'total_option_volume', 'log_moneyness_skew']].head())\n",
    "\n",
    "print(\"\\nTop 10 days by average absolute log-moneyness skew (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('date')['log_moneyness_skew']\n",
    "    .mean()\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 tickers by average absolute log-moneyness skew (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('underlying_ticker')['log_moneyness_skew']\n",
    "    .mean()\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nLog-Moneyness Skew Summary Statistics (for days with significant volume):\")\n",
    "skew_stats = clean_df[clean_df['total_option_volume'] > 1000]['log_moneyness_skew'].describe()\n",
    "print(skew_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/2539631142.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_expiry_variance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with expiry statistics:\n",
      "        date underlying_ticker  total_option_volume  weighted_mean_dte  \\\n",
      "0 2024-07-01                AA              18243.0          62.026750   \n",
      "1 2024-07-01               AAL             103084.0          65.054713   \n",
      "2 2024-07-01              AAOI               2974.0          69.885676   \n",
      "3 2024-07-01               AAP               6723.0          36.814517   \n",
      "4 2024-07-01              AAPL            1432971.0          30.496198   \n",
      "\n",
      "   weighted_std_dte  \n",
      "0         95.056529  \n",
      "1        106.963569  \n",
      "2         79.813993  \n",
      "3         89.482436  \n",
      "4         76.720547  \n",
      "\n",
      "Top 10 tickers by average DTE spread (minimum volume filter):\n",
      "underlying_ticker\n",
      "VALE    206.198355\n",
      "POET    197.246402\n",
      "BTG     196.104729\n",
      "DNN     184.346450\n",
      "EOSE    183.550141\n",
      "GRAB    180.726872\n",
      "TELL    179.236952\n",
      "SBSW    178.097077\n",
      "LAZR    176.317763\n",
      "LAC     173.035344\n",
      "Name: weighted_std_dte, dtype: float64\n",
      "\n",
      "DTE Statistics (for days with significant volume):\n",
      "       weighted_mean_dte  weighted_std_dte\n",
      "count       96106.000000      96106.000000\n",
      "mean           66.402634         91.007538\n",
      "std            53.707411         48.393351\n",
      "min             0.023795          0.000000\n",
      "25%            34.665378         58.513202\n",
      "50%            51.752315         81.166786\n",
      "75%            79.896670        112.460247\n",
      "max           791.476420        424.671511\n",
      "\n",
      "Correlations with DTE spread:\n",
      "total_option_volume   -0.01468\n",
      "option_hhi            -0.20174\n",
      "log_moneyness_skew    -0.06970\n",
      "weighted_std_dte       1.00000\n",
      "Name: weighted_std_dte, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def calculate_expiry_variance(group):\n",
    "    if len(group) == 0 or group['volume'].sum() == 0:\n",
    "        return pd.Series({\n",
    "            'weighted_mean_dte': 0,\n",
    "            'weighted_var_dte': 0\n",
    "        })\n",
    "    \n",
    "    # Calculate days to expiration for each contract\n",
    "    group = group.copy()  # Avoid SettingWithCopyWarning\n",
    "    group['dte'] = (pd.to_datetime(group['expiration_date']) - pd.to_datetime(group['trading_date'])).dt.days\n",
    "    \n",
    "    # Calculate volume weights\n",
    "    total_volume = group['volume'].sum()\n",
    "    volume_weights = group['volume'] / total_volume\n",
    "    \n",
    "    # Calculate weighted mean DTE\n",
    "    weighted_mean_dte = (group['dte'] * volume_weights).sum()\n",
    "    \n",
    "    # Calculate weighted variance\n",
    "    squared_deviations = (group['dte'] - weighted_mean_dte) ** 2\n",
    "    weighted_variance = (squared_deviations * volume_weights).sum()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'weighted_mean_dte': weighted_mean_dte,\n",
    "        'weighted_var_dte': weighted_variance\n",
    "    })\n",
    "\n",
    "# Calculate expiry variance for each ticker-day\n",
    "expiry_stats = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])\n",
    "    .apply(calculate_expiry_variance)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Normalize dates\n",
    "expiry_stats['trading_date'] = pd.to_datetime(expiry_stats['trading_date']).dt.normalize()\n",
    "expiry_stats = expiry_stats.rename(columns={'trading_date': 'date'})\n",
    "\n",
    "# Merge with clean_df\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    expiry_stats,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "clean_df['weighted_mean_dte'] = clean_df['weighted_mean_dte'].fillna(0)\n",
    "clean_df['weighted_var_dte'] = clean_df['weighted_var_dte'].fillna(0)\n",
    "\n",
    "# Add standard deviation column for easier interpretation\n",
    "clean_df['weighted_std_dte'] = np.sqrt(clean_df['weighted_var_dte'])\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Sample of data with expiry statistics:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'total_option_volume', \n",
    "                'weighted_mean_dte', 'weighted_std_dte']].head())\n",
    "\n",
    "print(\"\\nTop 10 tickers by average DTE spread (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('underlying_ticker')['weighted_std_dte']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nDTE Statistics (for days with significant volume):\")\n",
    "dte_stats = clean_df[clean_df['total_option_volume'] > 1000][\n",
    "    ['weighted_mean_dte', 'weighted_std_dte']\n",
    "].describe()\n",
    "print(dte_stats)\n",
    "\n",
    "# Optional: Look at correlation between DTE spread and other metrics\n",
    "print(\"\\nCorrelations with DTE spread:\")\n",
    "correlation_cols = ['total_option_volume', 'option_hhi', 'log_moneyness_skew', 'weighted_std_dte']\n",
    "correlations = clean_df[clean_df['total_option_volume'] > 1000][correlation_cols].corr()['weighted_std_dte']\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>underlying_ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>total_option_volume</th>\n",
       "      <th>option_hhi</th>\n",
       "      <th>volume_weighted_hhi</th>\n",
       "      <th>weighted_strike_distance</th>\n",
       "      <th>log_moneyness_skew</th>\n",
       "      <th>weighted_mean_dte</th>\n",
       "      <th>weighted_var_dte</th>\n",
       "      <th>weighted_std_dte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>40.021413</td>\n",
       "      <td>40.637890</td>\n",
       "      <td>39.315446</td>\n",
       "      <td>39.474537</td>\n",
       "      <td>3228000.0</td>\n",
       "      <td>18243.0</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>48811.239887</td>\n",
       "      <td>-0.685844</td>\n",
       "      <td>62.026750</td>\n",
       "      <td>9035.743625</td>\n",
       "      <td>95.056529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAL</td>\n",
       "      <td>11.330000</td>\n",
       "      <td>11.410000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.040000</td>\n",
       "      <td>28624300.0</td>\n",
       "      <td>103084.0</td>\n",
       "      <td>0.042765</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>15258.147886</td>\n",
       "      <td>-0.305484</td>\n",
       "      <td>65.054713</td>\n",
       "      <td>11441.205050</td>\n",
       "      <td>106.963569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>8.390000</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>8.080000</td>\n",
       "      <td>1778500.0</td>\n",
       "      <td>2974.0</td>\n",
       "      <td>0.148861</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>4045.293617</td>\n",
       "      <td>0.284867</td>\n",
       "      <td>69.885676</td>\n",
       "      <td>6370.273413</td>\n",
       "      <td>79.813993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAP</td>\n",
       "      <td>61.909940</td>\n",
       "      <td>62.147368</td>\n",
       "      <td>58.813453</td>\n",
       "      <td>59.070667</td>\n",
       "      <td>1767600.0</td>\n",
       "      <td>6723.0</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>64496.290477</td>\n",
       "      <td>-0.815526</td>\n",
       "      <td>36.814517</td>\n",
       "      <td>8007.106307</td>\n",
       "      <td>89.482436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>211.611974</td>\n",
       "      <td>217.019756</td>\n",
       "      <td>211.442359</td>\n",
       "      <td>216.261475</td>\n",
       "      <td>60402900.0</td>\n",
       "      <td>1432971.0</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>71137.721162</td>\n",
       "      <td>1.070264</td>\n",
       "      <td>30.496198</td>\n",
       "      <td>5886.042300</td>\n",
       "      <td>76.720547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date underlying_ticker        open        high         low  \\\n",
       "0 2024-07-01                AA   40.021413   40.637890   39.315446   \n",
       "1 2024-07-01               AAL   11.330000   11.410000   11.000000   \n",
       "2 2024-07-01              AAOI    8.390000    8.660000    7.850000   \n",
       "3 2024-07-01               AAP   61.909940   62.147368   58.813453   \n",
       "4 2024-07-01              AAPL  211.611974  217.019756  211.442359   \n",
       "\n",
       "        close      volume  total_option_volume  option_hhi  \\\n",
       "0   39.474537   3228000.0              18243.0    0.044966   \n",
       "1   11.040000  28624300.0             103084.0    0.042765   \n",
       "2    8.080000   1778500.0               2974.0    0.148861   \n",
       "3   59.070667   1767600.0               6723.0    0.027098   \n",
       "4  216.261475  60402900.0            1432971.0    0.033944   \n",
       "\n",
       "   volume_weighted_hhi  weighted_strike_distance  log_moneyness_skew  \\\n",
       "0             0.000033              48811.239887           -0.685844   \n",
       "1             0.000175              15258.147886           -0.305484   \n",
       "2             0.000018               4045.293617            0.284867   \n",
       "3             0.000007              64496.290477           -0.815526   \n",
       "4             0.001932              71137.721162            1.070264   \n",
       "\n",
       "   weighted_mean_dte  weighted_var_dte  weighted_std_dte  \n",
       "0          62.026750       9035.743625         95.056529  \n",
       "1          65.054713      11441.205050        106.963569  \n",
       "2          69.885676       6370.273413         79.813993  \n",
       "3          36.814517       8007.106307         89.482436  \n",
       "4          30.496198       5886.042300         76.720547  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/8mjnv1ts4lb9snv_c2g7gslh0000gn/T/ipykernel_34817/2356105829.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_os['os_ratio'] = daily_os.groupby('underlying_ticker').apply(calculate_os_ratio).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with O/S ratios:\n",
      "        date underlying_ticker      volume  total_option_volume    os_ratio  \\\n",
      "0 2024-07-01                AA   3228000.0              18243.0   70.113166   \n",
      "1 2024-07-01               AAL  28624300.0             103084.0   53.479809   \n",
      "2 2024-07-01              AAOI   1778500.0               2974.0   40.153725   \n",
      "3 2024-07-01               AAP   1767600.0               6723.0   67.741967   \n",
      "4 2024-07-01              AAPL  60402900.0            1432971.0  174.846532   \n",
      "\n",
      "   os_ratio_ma20  normalized_os_ratio  \n",
      "0            0.0                  0.0  \n",
      "1            0.0                  0.0  \n",
      "2            0.0                  0.0  \n",
      "3            0.0                  0.0  \n",
      "4            0.0                  0.0  \n",
      "\n",
      "Top 10 stocks by average O/S ratio (minimum volume filter):\n",
      "underlying_ticker\n",
      "MVIS     224.045436\n",
      "CLMT       9.371030\n",
      "TPR        8.813288\n",
      "GSM        6.319152\n",
      "XRX        5.562419\n",
      "MLCO       5.406618\n",
      "MCHP       4.507057\n",
      "NKLA1      4.028937\n",
      "OPTT       3.746929\n",
      "ROIV       3.535104\n",
      "Name: os_ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First calculate the raw O/S ratio for each ticker-day\n",
    "def calculate_os_ratio(group):\n",
    "    # Convert option volume to share equivalent (× 100)\n",
    "    option_share_volume = group['option_volume'].sum() * 100\n",
    "    # Get stock volume\n",
    "    stock_volume = group['stock_volume'].iloc[0]\n",
    "    # Calculate ratio\n",
    "    return option_share_volume / stock_volume if stock_volume > 0 else 0\n",
    "\n",
    "# Calculate daily O/S ratios\n",
    "daily_os = (\n",
    "    filtered_df.groupby(['trading_date', 'underlying_ticker'])['volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'volume': 'option_volume'})\n",
    ")\n",
    "\n",
    "# Merge with stock volume data\n",
    "daily_os = pd.merge(\n",
    "    daily_os,\n",
    "    clean_df[['date', 'underlying_ticker', 'volume']].rename(columns={'volume': 'stock_volume'}),\n",
    "    left_on=['trading_date', 'underlying_ticker'],\n",
    "    right_on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate raw O/S ratio\n",
    "daily_os['os_ratio'] = daily_os.groupby('underlying_ticker').apply(calculate_os_ratio).reset_index(drop=True)\n",
    "\n",
    "# Calculate 20-day moving average O/S ratio\n",
    "daily_os['date'] = pd.to_datetime(daily_os['date'])\n",
    "daily_os = daily_os.sort_values(['underlying_ticker', 'date'])\n",
    "daily_os['os_ratio_ma20'] = daily_os.groupby('underlying_ticker')['os_ratio'].transform(\n",
    "    lambda x: x.rolling(window=20, min_periods=5).mean()\n",
    ")\n",
    "\n",
    "# Calculate normalized O/S ratio\n",
    "daily_os['normalized_os_ratio'] = daily_os['os_ratio'] / daily_os['os_ratio_ma20']\n",
    "\n",
    "# Clean up and prepare for merge\n",
    "os_metrics = daily_os[['date', 'underlying_ticker', 'os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']]\n",
    "\n",
    "# Merge with clean_df\n",
    "clean_df = pd.merge(\n",
    "    clean_df,\n",
    "    os_metrics,\n",
    "    on=['date', 'underlying_ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "clean_df[['os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']] = clean_df[\n",
    "    ['os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']\n",
    "].fillna(0)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Sample of data with O/S ratios:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'volume', 'total_option_volume', \n",
    "                'os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']].head())\n",
    "\n",
    "print(\"\\nTop 10 stocks by average O/S ratio (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('underlying_ticker')['os_ratio']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class balance for different horizons and thresholds:\n",
      "\n",
      "5-day, 2.0% threshold:\n",
      "signal_5d_2pct\n",
      "1    0.616856\n",
      "0    0.383144\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "5-day, 3.0% threshold:\n",
      "signal_5d_3pct\n",
      "0    0.507632\n",
      "1    0.492368\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "5-day, 5.0% threshold:\n",
      "signal_5d_5pct\n",
      "0    0.684864\n",
      "1    0.315136\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "10-day, 2.0% threshold:\n",
      "signal_10d_2pct\n",
      "1    0.686448\n",
      "0    0.313552\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "10-day, 3.0% threshold:\n",
      "signal_10d_3pct\n",
      "1    0.591312\n",
      "0    0.408688\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "10-day, 5.0% threshold:\n",
      "signal_10d_5pct\n",
      "0    0.564944\n",
      "1    0.435056\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "20-day, 2.0% threshold:\n",
      "signal_20d_2pct\n",
      "1    0.68244\n",
      "0    0.31756\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "20-day, 3.0% threshold:\n",
      "signal_20d_3pct\n",
      "1    0.620032\n",
      "0    0.379968\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "20-day, 5.0% threshold:\n",
      "signal_20d_5pct\n",
      "1    0.506208\n",
      "0    0.493792\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Top 10 stocks by signal frequency (20-day, 2% threshold):\n",
      "underlying_ticker\n",
      "AGL     0.840\n",
      "CONL    0.840\n",
      "RKT     0.840\n",
      "BIIB    0.840\n",
      "ZETA    0.832\n",
      "HOOD    0.832\n",
      "RVNC    0.832\n",
      "GOEV    0.832\n",
      "SMR     0.832\n",
      "UAL     0.824\n",
      "Name: signal_20d_2pct, dtype: float64\n",
      "\n",
      "Correlation with option metrics (20-day, 2% threshold):\n",
      "total_option_volume    0.003638\n",
      "option_hhi             0.028793\n",
      "log_moneyness_skew     0.003148\n",
      "weighted_std_dte       0.011802\n",
      "normalized_os_ratio         NaN\n",
      "signal_20d_2pct        1.000000\n",
      "Name: signal_20d_2pct, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# First, let's get SPY data for market returns\n",
    "spy = yf.download(\"SPY\", \n",
    "                  start=clean_df['date'].min(),\n",
    "                  end=clean_df['date'].max() + pd.Timedelta(days=30))  # Extra 30 days for forward returns\n",
    "\n",
    "# Calculate forward returns for different horizons (e.g., 5, 10, 20 days)\n",
    "def calculate_forward_returns(df, horizons=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Calculate forward returns for multiple horizons\n",
    "    df: DataFrame with dates and tickers\n",
    "    horizons: List of forward days to calculate returns for\n",
    "    \"\"\"\n",
    "    # Calculate market returns for each horizon\n",
    "    for n in horizons:\n",
    "        spy['forward_ret_' + str(n)] = (\n",
    "            spy['Close'].shift(-n) / spy['Close'] - 1\n",
    "        )\n",
    "    \n",
    "    # For each stock and horizon\n",
    "    for n in horizons:\n",
    "        # Calculate stock's forward return\n",
    "        df['forward_ret_' + str(n)] = df.groupby('underlying_ticker')['close'].transform(\n",
    "            lambda x: x.shift(-n) / x - 1\n",
    "        )\n",
    "        \n",
    "        # Calculate excess return\n",
    "        df['excess_ret_' + str(n)] = (\n",
    "            df['forward_ret_' + str(n)] - \n",
    "            spy.loc[df['date']]['forward_ret_' + str(n)].values\n",
    "        )\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Function to create binary signals based on threshold\n",
    "def create_binary_signals(df, threshold, horizons=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Create binary signals for significant moves\n",
    "    threshold: Percentage threshold for significant moves (e.g., 0.02 for 2%)\n",
    "    \"\"\"\n",
    "    for n in horizons:\n",
    "        col_name = f'signal_{n}d_{int(threshold*100)}pct'\n",
    "        df[col_name] = (\n",
    "            (df['excess_ret_' + str(n)].abs() > threshold)\n",
    "            .astype(int)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Apply the calculations\n",
    "horizons = [5, 10, 20]  # Different time horizons to consider\n",
    "thresholds = [0.02, 0.03, 0.05]  # Different thresholds to test\n",
    "\n",
    "# Calculate returns and signals for all stocks at once\n",
    "clean_df = calculate_forward_returns(clean_df, horizons)\n",
    "\n",
    "# Create signals for different thresholds\n",
    "for threshold in thresholds:\n",
    "    clean_df = create_binary_signals(clean_df, threshold, horizons)\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display class balance for different horizons and thresholds\n",
    "print(\"\\nClass balance for different horizons and thresholds:\")\n",
    "for n in horizons:\n",
    "    for threshold in thresholds:\n",
    "        col_name = f'signal_{n}d_{int(threshold*100)}pct'\n",
    "        signal_dist = clean_df[col_name].value_counts(normalize=True)\n",
    "        print(f\"\\n{n}-day, {threshold*100}% threshold:\")\n",
    "        print(signal_dist)\n",
    "        \n",
    "# Calculate average signal rate by ticker\n",
    "print(\"\\nTop 10 stocks by signal frequency (20-day, 2% threshold):\")\n",
    "signal_col = 'signal_20d_2pct'\n",
    "print(\n",
    "    clean_df.groupby('underlying_ticker')[signal_col]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Optional: Show correlation with our option metrics\n",
    "print(\"\\nCorrelation with option metrics (20-day, 2% threshold):\")\n",
    "correlation_cols = ['total_option_volume', 'option_hhi', 'log_moneyness_skew', \n",
    "                   'weighted_std_dte', 'normalized_os_ratio', signal_col]\n",
    "correlations = clean_df[correlation_cols].corr()[signal_col]\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized O/S Ratio Statistics:\n",
      "count    125000.0\n",
      "mean          0.0\n",
      "std           0.0\n",
      "min           0.0\n",
      "25%           0.0\n",
      "50%           0.0\n",
      "75%           0.0\n",
      "max           0.0\n",
      "Name: normalized_os_ratio, dtype: float64\n",
      "\n",
      "NaN count in normalized_os_ratio: 0\n",
      "\n",
      "Infinite values in normalized_os_ratio: 0\n",
      "\n",
      "Top 10 highest normalized_os_ratio values:\n",
      "        date underlying_ticker    os_ratio  os_ratio_ma20  normalized_os_ratio\n",
      "0 2024-07-01                AA   70.113166            0.0                  0.0\n",
      "1 2024-07-01               AAL   53.479809            0.0                  0.0\n",
      "2 2024-07-01              AAOI   40.153725            0.0                  0.0\n",
      "3 2024-07-01               AAP   67.741967            0.0                  0.0\n",
      "4 2024-07-01              AAPL  174.846532            0.0                  0.0\n",
      "5 2024-07-01              ABBV   49.140360            0.0                  0.0\n",
      "6 2024-07-01              ABNB  134.106093            0.0                  0.0\n",
      "7 2024-07-01               ABR   46.807496            0.0                  0.0\n",
      "8 2024-07-01               ABT   33.285406            0.0                  0.0\n",
      "9 2024-07-01               ACB   99.934491            0.0                  0.0\n",
      "\n",
      "Sample of rows where normalized_os_ratio is NaN:\n",
      "Empty DataFrame\n",
      "Columns: [date, underlying_ticker, os_ratio, os_ratio_ma20, normalized_os_ratio]\n",
      "Index: []\n",
      "\n",
      "Statistics for components:\n",
      "\n",
      "os_ratio:\n",
      "count    125000.000000\n",
      "mean          0.618356\n",
      "std          58.491155\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max       20388.134715\n",
      "Name: os_ratio, dtype: float64\n",
      "\n",
      "os_ratio_ma20:\n",
      "count    125000.0\n",
      "mean          0.0\n",
      "std           0.0\n",
      "min           0.0\n",
      "25%           0.0\n",
      "50%           0.0\n",
      "75%           0.0\n",
      "max           0.0\n",
      "Name: os_ratio_ma20, dtype: float64\n",
      "\n",
      "Count of zero values in os_ratio_ma20: 125000\n",
      "\n",
      "Value ranges for normalized_os_ratio:\n",
      "normalized_os_ratio\n",
      "(-inf, 0.0]    125000\n",
      "(0.0, 1.0]          0\n",
      "(1.0, 2.0]          0\n",
      "(2.0, 5.0]          0\n",
      "(5.0, 10.0]         0\n",
      "(10.0, inf]         0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics for normalized_os_ratio\n",
    "print(\"Normalized O/S Ratio Statistics:\")\n",
    "print(clean_df['normalized_os_ratio'].describe())\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"\\nNaN count in normalized_os_ratio:\", clean_df['normalized_os_ratio'].isna().sum())\n",
    "\n",
    "# Check for infinite values\n",
    "print(\"\\nInfinite values in normalized_os_ratio:\", \n",
    "      clean_df['normalized_os_ratio'].isin([np.inf, -np.inf]).sum())\n",
    "\n",
    "# Look at some extreme values\n",
    "print(\"\\nTop 10 highest normalized_os_ratio values:\")\n",
    "print(clean_df.nlargest(10, 'normalized_os_ratio')[\n",
    "    ['date', 'underlying_ticker', 'os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']\n",
    "])\n",
    "\n",
    "print(\"\\nSample of rows where normalized_os_ratio is NaN:\")\n",
    "print(clean_df[clean_df['normalized_os_ratio'].isna()][\n",
    "    ['date', 'underlying_ticker', 'os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']\n",
    "].head())\n",
    "\n",
    "# Check the components\n",
    "print(\"\\nStatistics for components:\")\n",
    "print(\"\\nos_ratio:\")\n",
    "print(clean_df['os_ratio'].describe())\n",
    "print(\"\\nos_ratio_ma20:\")\n",
    "print(clean_df['os_ratio_ma20'].describe())\n",
    "\n",
    "# Check for zero values in ma20 (which would cause division by zero)\n",
    "print(\"\\nCount of zero values in os_ratio_ma20:\", (clean_df['os_ratio_ma20'] == 0).sum())\n",
    "\n",
    "# Distribution of values\n",
    "print(\"\\nValue ranges for normalized_os_ratio:\")\n",
    "ranges = [-np.inf, 0, 1, 2, 5, 10, np.inf]\n",
    "print(pd.cut(clean_df['normalized_os_ratio'], bins=ranges).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data with O/S ratios:\n",
      "           date underlying_ticker     volume  total_option_volume  os_ratio  \\\n",
      "0    2024-07-01                AA  3228000.0              18243.0  0.565149   \n",
      "1000 2024-07-02                AA  4039500.0              35060.0  0.867929   \n",
      "2000 2024-07-03                AA  4074500.0              32198.0  0.790232   \n",
      "3000 2024-07-05                AA  2670200.0              14792.0  0.553966   \n",
      "4000 2024-07-08                AA  3001000.0              12452.0  0.414928   \n",
      "\n",
      "      os_ratio_ma20  normalized_os_ratio  \n",
      "0               NaN             0.000000  \n",
      "1000            NaN             0.000000  \n",
      "2000            NaN             0.000000  \n",
      "3000            NaN             0.000000  \n",
      "4000       0.638441             0.649909  \n",
      "\n",
      "Top 10 stocks by average O/S ratio (minimum volume filter):\n",
      "underlying_ticker\n",
      "MATV    3.961949\n",
      "CRTO    3.508786\n",
      "SIRI    3.097998\n",
      "NFLX    2.980975\n",
      "META    2.723343\n",
      "COST    2.646241\n",
      "CVNA    2.486378\n",
      "GME     2.329859\n",
      "TSLA    2.273751\n",
      "COIN    2.225719\n",
      "Name: os_ratio, dtype: float64\n",
      "\n",
      "O/S Ratio Statistics:\n",
      "           os_ratio  os_ratio_ma20  normalized_os_ratio\n",
      "count  96106.000000   93366.000000         96106.000000\n",
      "mean       0.376612       0.357297             1.112704\n",
      "std        0.618700       0.437593             1.010150\n",
      "min        0.000000       0.000000             0.000000\n",
      "25%        0.098702       0.113310             0.622056\n",
      "50%        0.212902       0.223964             0.908940\n",
      "75%        0.446360       0.435928             1.304861\n",
      "max       65.568970      14.274496            20.000000\n",
      "\n",
      "Distribution of normalized O/S ratios (for significant volume):\n",
      "normalized_os_ratio\n",
      "(0.0, 0.5]     11340\n",
      "(0.5, 1.0]     39512\n",
      "(1.0, 2.0]     32075\n",
      "(2.0, 5.0]      7986\n",
      "(5.0, 10.0]      837\n",
      "(10.0, inf]      154\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First calculate the raw O/S ratio for each ticker-day\n",
    "def calculate_os_ratio(group):\n",
    "    # Convert option volume to share equivalent (× 100)\n",
    "    option_share_volume = group['total_option_volume'] * 100\n",
    "    # Get stock volume\n",
    "    stock_volume = group['volume']\n",
    "    # Calculate ratio\n",
    "    return option_share_volume / stock_volume if stock_volume > 0 else 0\n",
    "\n",
    "# Calculate daily O/S ratios\n",
    "clean_df['os_ratio'] = clean_df.apply(\n",
    "    lambda row: calculate_os_ratio(row), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sort by date for proper rolling calculations\n",
    "clean_df = clean_df.sort_values(['underlying_ticker', 'date'])\n",
    "\n",
    "# Calculate 20-day moving average O/S ratio\n",
    "clean_df['os_ratio_ma20'] = clean_df.groupby('underlying_ticker')['os_ratio'].transform(\n",
    "    lambda x: x.rolling(window=20, min_periods=5).mean()\n",
    ")\n",
    "\n",
    "# Calculate normalized O/S ratio\n",
    "clean_df['normalized_os_ratio'] = clean_df.apply(\n",
    "    lambda row: row['os_ratio'] / row['os_ratio_ma20'] if row['os_ratio_ma20'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save updated dataset\n",
    "clean_df.to_csv('data/processed/day_aggs/clean_market_data.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Sample of data with O/S ratios:\")\n",
    "print(clean_df[['date', 'underlying_ticker', 'volume', 'total_option_volume', \n",
    "                'os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']].head())\n",
    "\n",
    "print(\"\\nTop 10 stocks by average O/S ratio (minimum volume filter):\")\n",
    "print(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]\n",
    "    .groupby('underlying_ticker')['os_ratio']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nO/S Ratio Statistics:\")\n",
    "print(clean_df[clean_df['total_option_volume'] > 1000][\n",
    "    ['os_ratio', 'os_ratio_ma20', 'normalized_os_ratio']\n",
    "].describe())\n",
    "\n",
    "# Distribution of normalized ratios\n",
    "print(\"\\nDistribution of normalized O/S ratios (for significant volume):\")\n",
    "ranges = [0, 0.5, 1, 2, 5, 10, float('inf')]\n",
    "print(pd.cut(\n",
    "    clean_df[clean_df['total_option_volume'] > 1000]['normalized_os_ratio'],\n",
    "    bins=ranges\n",
    ").value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset for signal_5d_3pct:\n",
      "Shape: (91904, 6)\n",
      "\n",
      "Class balance:\n",
      "signal_5d_3pct\n",
      "1    0.508814\n",
      "0    0.491186\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature correlations with target:\n",
      "signal_5d_3pct         1.000000\n",
      "total_option_volume    0.016676\n",
      "normalized_os_ratio    0.011137\n",
      "weighted_std_dte       0.004773\n",
      "option_hhi            -0.014807\n",
      "log_moneyness_skew    -0.038395\n",
      "Name: signal_5d_3pct, dtype: float64\n",
      "\n",
      "Dataset for signal_10d_3pct:\n",
      "Shape: (91904, 6)\n",
      "\n",
      "Class balance:\n",
      "signal_10d_3pct\n",
      "1    0.607144\n",
      "0    0.392856\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature correlations with target:\n",
      "signal_10d_3pct        1.000000\n",
      "total_option_volume    0.010458\n",
      "normalized_os_ratio    0.008902\n",
      "option_hhi             0.002206\n",
      "log_moneyness_skew    -0.000957\n",
      "weighted_std_dte      -0.010353\n",
      "Name: signal_10d_3pct, dtype: float64\n",
      "\n",
      "Dataset for signal_20d_3pct:\n",
      "Shape: (91904, 6)\n",
      "\n",
      "Class balance:\n",
      "signal_20d_3pct\n",
      "1    0.632704\n",
      "0    0.367296\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature correlations with target:\n",
      "signal_20d_3pct        1.000000\n",
      "normalized_os_ratio    0.021469\n",
      "option_hhi             0.005150\n",
      "total_option_volume    0.001009\n",
      "log_moneyness_skew    -0.002351\n",
      "weighted_std_dte      -0.027158\n",
      "Name: signal_20d_3pct, dtype: float64\n",
      "\n",
      "Sample of modeling data:\n",
      "      total_option_volume  option_hhi  log_moneyness_skew  weighted_std_dte  \\\n",
      "4000              12452.0    0.032215            1.031624         45.105411   \n",
      "5000              14028.0    0.076732            1.056828         35.082979   \n",
      "6000              22845.0    0.026009           -0.717577         60.936615   \n",
      "7000              16075.0    0.030153           -0.036636         55.671825   \n",
      "8000              23234.0    0.140261            0.128778         46.796685   \n",
      "\n",
      "      normalized_os_ratio  signal_5d_3pct  signal_10d_3pct  signal_20d_3pct  \n",
      "4000             0.649909               1                1                1  \n",
      "5000             0.750303               1                1                1  \n",
      "6000             0.649016               1                1                1  \n",
      "7000             0.347443               1                1                1  \n",
      "8000             0.917099               1                1                1  \n"
     ]
    }
   ],
   "source": [
    "# Create feature set with our key metrics\n",
    "features = [\n",
    "    'total_option_volume',  # Overall option activity\n",
    "    'option_hhi',          # Concentration of activity\n",
    "    'log_moneyness_skew',  # Put vs call strike positioning\n",
    "    'weighted_std_dte',    # Time horizon spread\n",
    "    'normalized_os_ratio'  # Option vs stock volume\n",
    "]\n",
    "\n",
    "# Target variables (using 3% threshold at different horizons)\n",
    "targets = [\n",
    "    'signal_5d_3pct',\n",
    "    'signal_10d_3pct', \n",
    "    'signal_20d_3pct'\n",
    "]\n",
    "\n",
    "# Create datasets for each prediction horizon\n",
    "for target in targets:\n",
    "    # Filter for minimum volume and valid ratios\n",
    "    model_data = clean_df[\n",
    "        (clean_df['total_option_volume'] > 1000) &  # Significant option activity\n",
    "        (clean_df['os_ratio_ma20'] > 0)            # Valid O/S ratio\n",
    "    ].copy()\n",
    "    \n",
    "    # Add any derived features\n",
    "    model_data['log_option_volume'] = np.log1p(model_data['total_option_volume'])\n",
    "    \n",
    "    print(f\"\\nDataset for {target}:\")\n",
    "    print(\"Shape:\", model_data[features + [target]].shape)\n",
    "    print(\"\\nClass balance:\")\n",
    "    print(model_data[target].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nFeature correlations with target:\")\n",
    "    correlations = model_data[features + [target]].corr()[target].sort_values(ascending=False)\n",
    "    print(correlations)\n",
    "    \n",
    "    # Save modeling dataset\n",
    "    model_data[features + [target] + ['date', 'underlying_ticker']].to_csv(\n",
    "        f'data/processed/model_data_{target}.csv',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# Display sample of final modeling data\n",
    "print(\"\\nSample of modeling data:\")\n",
    "print(model_data[features + targets].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding enhanced features...\n",
      "\n",
      "Evaluating Logistic Regression\n",
      "\n",
      "Evaluating Linear SVM\n",
      "\n",
      "Evaluating Neural Network\n",
      "\n",
      "Evaluating Random Forest\n",
      "\n",
      "=== Logistic Regression Results ===\n",
      "Average ROC AUC: 0.5428\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-08T00:00:00.000000000 to 2024-11-26T00:00:00.000000000\n",
      "Val: 2024-11-26T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.559898  0.474317  0.513567   8819.000000\n",
      "1              0.409126  0.493998  0.447574   6498.000000\n",
      "accuracy       0.482666  0.482666  0.482666      0.482666\n",
      "macro avg      0.484512  0.484157  0.480570  15317.000000\n",
      "weighted avg   0.495935  0.482666  0.485570  15317.000000\n",
      "\n",
      "=== Linear SVM Results ===\n",
      "Average ROC AUC: 0.5428\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-08T00:00:00.000000000 to 2024-11-26T00:00:00.000000000\n",
      "Val: 2024-11-26T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.559705  0.474090  0.513353   8819.00000\n",
      "1              0.408946  0.493844  0.447403   6498.00000\n",
      "accuracy       0.482470  0.482470  0.482470      0.48247\n",
      "macro avg      0.484326  0.483967  0.480378  15317.00000\n",
      "weighted avg   0.495748  0.482470  0.485375  15317.00000\n",
      "\n",
      "=== Neural Network Results ===\n",
      "Average ROC AUC: 0.5601\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-08T00:00:00.000000000 to 2024-11-26T00:00:00.000000000\n",
      "Val: 2024-11-26T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.589460  0.417281  0.488647   8819.00000\n",
      "1              0.433657  0.605571  0.505394   6498.00000\n",
      "accuracy       0.497160  0.497160  0.497160      0.49716\n",
      "macro avg      0.511558  0.511426  0.497021  15317.00000\n",
      "weighted avg   0.523363  0.497160  0.495752  15317.00000\n",
      "\n",
      "=== Random Forest Results ===\n",
      "Average ROC AUC: 0.5525\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-08T00:00:00.000000000 to 2024-11-26T00:00:00.000000000\n",
      "Val: 2024-11-26T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.577901  0.415126  0.483173   8819.000000\n",
      "1              0.425740  0.588489  0.494057   6498.000000\n",
      "accuracy       0.488673  0.488673  0.488673      0.488673\n",
      "macro avg      0.501820  0.501808  0.488615  15317.000000\n",
      "weighted avg   0.513349  0.488673  0.487790  15317.000000\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "                feature  importance\n",
      "11           skew_trend    0.273116\n",
      "2    log_moneyness_skew    0.116371\n",
      "6           volume_skew    0.095496\n",
      "3      weighted_std_dte    0.064635\n",
      "1            option_hhi    0.062675\n",
      "7         volume_spread    0.056783\n",
      "8   concentration_ratio    0.055632\n",
      "9           rolling_vol    0.052538\n",
      "0   total_option_volume    0.049043\n",
      "5     log_option_volume    0.048077\n",
      "12       os_ratio_trend    0.046250\n",
      "4   normalized_os_ratio    0.042042\n",
      "10           vol_change    0.037342\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "print(\"Adding enhanced features...\")\n",
    "\n",
    "# 1. Interaction terms\n",
    "model_data['volume_skew'] = model_data['total_option_volume'] * model_data['log_moneyness_skew']\n",
    "model_data['volume_spread'] = model_data['total_option_volume'] * model_data['weighted_std_dte']\n",
    "model_data['concentration_ratio'] = model_data['normalized_os_ratio'] * model_data['option_hhi']\n",
    "\n",
    "# 2. Temporal features\n",
    "# Sort for proper rolling calculations\n",
    "model_data = model_data.sort_values(['underlying_ticker', 'date'])\n",
    "\n",
    "# Rolling statistics\n",
    "model_data['rolling_vol'] = model_data.groupby('underlying_ticker')['total_option_volume'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).std()\n",
    ")\n",
    "model_data['vol_change'] = model_data.groupby('underlying_ticker')['total_option_volume'].transform(\n",
    "    lambda x: x.pct_change()\n",
    ")\n",
    "\n",
    "# 3. Relative measures\n",
    "model_data['skew_trend'] = model_data.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    ")\n",
    "model_data['os_ratio_trend'] = model_data.groupby('underlying_ticker')['normalized_os_ratio'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Updated feature list\n",
    "features = [\n",
    "    # Original features\n",
    "    'total_option_volume',\n",
    "    'option_hhi',\n",
    "    'log_moneyness_skew',\n",
    "    'weighted_std_dte',\n",
    "    'normalized_os_ratio',\n",
    "    'log_option_volume',\n",
    "    # Interaction features\n",
    "    'volume_skew',\n",
    "    'volume_spread',\n",
    "    'concentration_ratio',\n",
    "    # Temporal features\n",
    "    'rolling_vol',\n",
    "    'vol_change',\n",
    "    'skew_trend',\n",
    "    'os_ratio_trend'\n",
    "]\n",
    "\n",
    "# Handle any NaN values from rolling calculations\n",
    "model_data = model_data.fillna(0)\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup\n",
    "target = 'signal_5d_3pct'\n",
    "\n",
    "# Prepare data\n",
    "model_data = model_data.sort_values('date')\n",
    "X = model_data[features].values\n",
    "y = model_data[target].values\n",
    "dates = model_data['date'].values\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        dual='auto'\n",
    "    ),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 25),\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Time series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Results storage\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        \n",
    "        # Handle different model types for probabilities\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "        else:  # For LinearSVC\n",
    "            y_pred_score = model.decision_function(X_val_scaled)\n",
    "            auc_score = roc_auc_score(y_val, y_pred_score)\n",
    "        \n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_dates': (dates[train_idx].min(), dates[train_idx].max()),\n",
    "            'val_dates': (dates[val_idx].min(), dates[val_idx].max()),\n",
    "            'roc_auc': auc_score,\n",
    "            'report': classification_report(y_val, y_pred, output_dict=True)\n",
    "        })\n",
    "    \n",
    "    all_results[name] = fold_metrics\n",
    "\n",
    "# Print results\n",
    "for name, results in all_results.items():\n",
    "    print(f\"\\n=== {name} Results ===\")\n",
    "    avg_auc = np.mean([fold['roc_auc'] for fold in results])\n",
    "    print(f\"Average ROC AUC: {avg_auc:.4f}\")\n",
    "    \n",
    "    # Print last fold details\n",
    "    last_fold = results[-1]\n",
    "    print(f\"\\nLast Fold Results:\")\n",
    "    print(f\"Train: {last_fold['train_dates'][0]} to {last_fold['train_dates'][1]}\")\n",
    "    print(f\"Val: {last_fold['val_dates'][0]} to {last_fold['val_dates'][1]}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pd.DataFrame(last_fold['report']).transpose())\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "if 'Random Forest' in models:\n",
    "    rf_model = models['Random Forest']\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    })\n",
    "    print(\"\\nFeature Importance (Random Forest):\")\n",
    "    print(importance.sort_values('importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more skew-related features\n",
    "model_data['skew_acceleration'] = model_data.groupby('underlying_ticker')['skew_trend'].transform(\n",
    "    lambda x: x.pct_change()\n",
    ")\n",
    "\n",
    "model_data['skew_volatility'] = model_data.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "    lambda x: x.rolling(window=5).std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multiple lookback periods\n",
    "for window in [3, 5, 10]:\n",
    "    model_data[f'skew_trend_{window}d'] = model_data.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add non-linear transformations of top features\n",
    "model_data['skew_trend_squared'] = model_data['skew_trend'] ** 2\n",
    "model_data['skew_trend_sign'] = np.sign(model_data['skew_trend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (91904, 48)\n",
      "Shape after dropping NaNs: (88000, 48)\n",
      "\n",
      "=== Evaluating Neural Network for signal_5d_3pct ===\n",
      "\n",
      "Results for signal_5d_3pct:\n",
      "Average ROC AUC: 0.5659\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-12T00:00:00.000000000 to 2024-11-27T00:00:00.000000000\n",
      "Val: 2024-11-27T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.620064  0.477067  0.539247   8525.000000\n",
      "1              0.450105  0.594203  0.512212   6141.000000\n",
      "accuracy       0.526115  0.526115  0.526115      0.526115\n",
      "macro avg      0.535084  0.535635  0.525730  14666.000000\n",
      "weighted avg   0.548898  0.526115  0.527927  14666.000000\n",
      "\n",
      "Top 10 Feature Correlations:\n",
      "                feature  correlation\n",
      "17       skew_trend_10d    -0.076069\n",
      "14      skew_volatility    -0.061494\n",
      "11           skew_trend    -0.061386\n",
      "16        skew_trend_5d    -0.061386\n",
      "19      skew_trend_sign    -0.058722\n",
      "15        skew_trend_3d    -0.052489\n",
      "2    log_moneyness_skew    -0.038632\n",
      "5     log_option_volume     0.038019\n",
      "7         volume_spread     0.018119\n",
      "0   total_option_volume     0.017262\n",
      "\n",
      "=== Evaluating Neural Network for signal_10d_3pct ===\n",
      "\n",
      "Results for signal_10d_3pct:\n",
      "Average ROC AUC: 0.5351\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-12T00:00:00.000000000 to 2024-11-27T00:00:00.000000000\n",
      "Val: 2024-11-27T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.618705  0.009059  0.017857   9493.000000\n",
      "1              0.352447  0.989754  0.519797   5173.000000\n",
      "accuracy       0.354971  0.354971  0.354971      0.354971\n",
      "macro avg      0.485576  0.499407  0.268827  14666.000000\n",
      "weighted avg   0.524790  0.354971  0.194902  14666.000000\n",
      "\n",
      "Top 10 Feature Correlations:\n",
      "                feature  correlation\n",
      "14      skew_volatility    -0.058260\n",
      "17       skew_trend_10d    -0.037843\n",
      "19      skew_trend_sign    -0.022003\n",
      "18   skew_trend_squared    -0.021886\n",
      "5     log_option_volume     0.021609\n",
      "11           skew_trend    -0.017483\n",
      "16        skew_trend_5d    -0.017483\n",
      "12       os_ratio_trend     0.012881\n",
      "0   total_option_volume     0.010996\n",
      "7         volume_spread     0.010785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Setup for both horizons\n",
    "targets = ['signal_5d_3pct', 'signal_10d_3pct']\n",
    "features = [\n",
    "    # Original features\n",
    "    'total_option_volume', 'option_hhi', 'log_moneyness_skew',\n",
    "    'weighted_std_dte', 'normalized_os_ratio', 'log_option_volume',\n",
    "    # Interaction features\n",
    "    'volume_skew', 'volume_spread', 'concentration_ratio',\n",
    "    # Temporal features\n",
    "    'rolling_vol', 'vol_change', 'skew_trend', 'os_ratio_trend',\n",
    "    # New skew features\n",
    "    'skew_acceleration', 'skew_volatility',\n",
    "    'skew_trend_3d', 'skew_trend_5d', 'skew_trend_10d',\n",
    "    'skew_trend_squared', 'skew_trend_sign'\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "print(\"Original shape:\", model_data.shape)\n",
    "model_data = model_data.dropna(subset=features + targets)\n",
    "print(\"Shape after dropping NaNs:\", model_data.shape)\n",
    "\n",
    "# Initialize model with slightly larger architecture\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    learning_rate_init=0.001,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# Evaluate for each horizon\n",
    "for target in targets:\n",
    "    print(f\"\\n=== Evaluating Neural Network for {target} ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    model_data = model_data.sort_values('date')\n",
    "    X = model_data[features].values\n",
    "    y = model_data[target].values\n",
    "    dates = model_data['date'].values\n",
    "    \n",
    "    # Time series cross validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Results storage\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Evaluate each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        \n",
    "        # Store metrics\n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_dates': (dates[train_idx].min(), dates[train_idx].max()),\n",
    "            'val_dates': (dates[val_idx].min(), dates[val_idx].max()),\n",
    "            'roc_auc': roc_auc_score(y_val, y_pred_proba),\n",
    "            'report': classification_report(y_val, y_pred, output_dict=True)\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {target}:\")\n",
    "    avg_auc = np.mean([fold['roc_auc'] for fold in fold_metrics])\n",
    "    print(f\"Average ROC AUC: {avg_auc:.4f}\")\n",
    "    \n",
    "    # Print last fold details\n",
    "    last_fold = fold_metrics[-1]\n",
    "    print(f\"\\nLast Fold Results:\")\n",
    "    print(f\"Train: {last_fold['train_dates'][0]} to {last_fold['train_dates'][1]}\")\n",
    "    print(f\"Val: {last_fold['val_dates'][0]} to {last_fold['val_dates'][1]}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pd.DataFrame(last_fold['report']).transpose())\n",
    "    \n",
    "    # Print feature correlations with target\n",
    "    correlations = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'correlation': [np.corrcoef(model_data[f], model_data[target])[0,1] for f in features]\n",
    "    })\n",
    "    print(\"\\nTop 10 Feature Correlations:\")\n",
    "    print(correlations.sort_values('correlation', key=abs, ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding enhanced features...\n",
      "Original shape: (88000, 55)\n",
      "Shape after dropping NaNs: (87024, 55)\n",
      "\n",
      "Training fold 1/5...\n",
      "\n",
      "Training fold 2/5...\n",
      "\n",
      "Training fold 3/5...\n",
      "\n",
      "Training fold 4/5...\n",
      "\n",
      "Training fold 5/5...\n",
      "\n",
      "Overall Results:\n",
      "Average ROC AUC: 0.5565\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-15T00:00:00.000000000 to 2024-11-27T00:00:00.000000000\n",
      "Val: 2024-11-27T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.589177  0.387078  0.467208   8466.000000\n",
      "1              0.419705  0.621563  0.501068   6038.000000\n",
      "accuracy       0.484694  0.484694  0.484694      0.484694\n",
      "macro avg      0.504441  0.504321  0.484138  14504.000000\n",
      "weighted avg   0.518626  0.484694  0.481304  14504.000000\n",
      "\n",
      "Top 15 Feature Correlations:\n",
      "                 feature  correlation\n",
      "17        skew_trend_10d    -0.075955\n",
      "22  skew_vol_interaction    -0.062571\n",
      "14       skew_volatility    -0.062239\n",
      "16         skew_trend_5d    -0.061153\n",
      "11            skew_trend    -0.061153\n",
      "19       skew_trend_sign    -0.058809\n",
      "15         skew_trend_3d    -0.052199\n",
      "2     log_moneyness_skew    -0.038137\n",
      "5      log_option_volume     0.038055\n",
      "7          volume_spread     0.018416\n",
      "1             option_hhi    -0.017543\n",
      "0    total_option_volume     0.017523\n",
      "9            rolling_vol     0.014014\n",
      "21         skew_momentum    -0.013548\n",
      "12        os_ratio_trend     0.009950\n"
     ]
    }
   ],
   "source": [
    "# Add more sophisticated feature interactions\n",
    "print(\"Adding enhanced features...\")\n",
    "\n",
    "# 1. Skew-based ratios and differences\n",
    "model_data['skew_trend_ratio'] = model_data['skew_trend_10d'] / model_data['skew_trend_5d']\n",
    "model_data['skew_momentum'] = model_data['skew_trend_10d'] - model_data['skew_trend_5d']\n",
    "model_data['skew_vol_interaction'] = model_data['skew_volatility'] * model_data['skew_trend_10d']\n",
    "\n",
    "# 2. Volume-skew interactions\n",
    "model_data['vol_skew_ratio'] = model_data['skew_volatility'] / model_data['skew_trend']\n",
    "model_data['vol_weighted_skew'] = model_data['total_option_volume'] * model_data['skew_trend']\n",
    "\n",
    "# 3. Trend acceleration\n",
    "model_data['skew_acceleration_3d'] = model_data.groupby('underlying_ticker')['skew_trend_3d'].transform(\n",
    "    lambda x: x.pct_change()\n",
    ")\n",
    "model_data['skew_acceleration_5d'] = model_data.groupby('underlying_ticker')['skew_trend_5d'].transform(\n",
    "    lambda x: x.pct_change()\n",
    ")\n",
    "\n",
    "# Updated feature list\n",
    "features = [\n",
    "    # Original features\n",
    "    'total_option_volume', 'option_hhi', 'log_moneyness_skew',\n",
    "    'weighted_std_dte', 'normalized_os_ratio', 'log_option_volume',\n",
    "    # Interaction features\n",
    "    'volume_skew', 'volume_spread', 'concentration_ratio',\n",
    "    # Temporal features\n",
    "    'rolling_vol', 'vol_change', 'skew_trend', 'os_ratio_trend',\n",
    "    # Skew features\n",
    "    'skew_acceleration', 'skew_volatility',\n",
    "    'skew_trend_3d', 'skew_trend_5d', 'skew_trend_10d',\n",
    "    'skew_trend_squared', 'skew_trend_sign',\n",
    "    # New features\n",
    "    'skew_trend_ratio', 'skew_momentum', 'skew_vol_interaction',\n",
    "    'vol_skew_ratio', 'vol_weighted_skew',\n",
    "    'skew_acceleration_3d', 'skew_acceleration_5d'\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "print(\"Original shape:\", model_data.shape)\n",
    "model_data = model_data.dropna(subset=features + ['signal_5d_3pct'])\n",
    "print(\"Shape after dropping NaNs:\", model_data.shape)\n",
    "\n",
    "# Try a deeper network with dropout and regularization\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize model with enhanced architecture\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(200, 100, 50),  # Deeper network\n",
    "    max_iter=2000,                      # More iterations\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    learning_rate_init=0.0005,          # Lower learning rate\n",
    "    batch_size=128,                     # Smaller batch size\n",
    "    alpha=0.0001,                       # L2 regularization\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    validation_fraction=0.15,           # Larger validation set\n",
    "    n_iter_no_change=20                 # More patience\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "X = model_data[features].values\n",
    "y = model_data['signal_5d_3pct'].values\n",
    "dates = model_data['date'].values\n",
    "\n",
    "# Time series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Results storage\n",
    "fold_metrics = []\n",
    "\n",
    "# Evaluate each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    print(f\"\\nTraining fold {fold+1}/5...\")\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train and predict\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'train_dates': (dates[train_idx].min(), dates[train_idx].max()),\n",
    "        'val_dates': (dates[val_idx].min(), dates[val_idx].max()),\n",
    "        'roc_auc': roc_auc_score(y_val, y_pred_proba),\n",
    "        'report': classification_report(y_val, y_pred, output_dict=True)\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOverall Results:\")\n",
    "avg_auc = np.mean([fold['roc_auc'] for fold in fold_metrics])\n",
    "print(f\"Average ROC AUC: {avg_auc:.4f}\")\n",
    "\n",
    "# Print last fold details\n",
    "last_fold = fold_metrics[-1]\n",
    "print(f\"\\nLast Fold Results:\")\n",
    "print(f\"Train: {last_fold['train_dates'][0]} to {last_fold['train_dates'][1]}\")\n",
    "print(f\"Val: {last_fold['val_dates'][0]} to {last_fold['val_dates'][1]}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(pd.DataFrame(last_fold['report']).transpose())\n",
    "\n",
    "# Feature importance analysis\n",
    "correlations = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'correlation': [np.corrcoef(model_data[f], model_data['signal_5d_3pct'])[0,1] for f in features]\n",
    "})\n",
    "print(\"\\nTop 15 Feature Correlations:\")\n",
    "print(correlations.sort_values('correlation', key=abs, ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced features...\n",
      "\n",
      "=== Testing Simple NN ===\n",
      "\n",
      "Testing with base features:\n",
      "\n",
      "Running experiment with 5 features\n",
      "Shape after dropping NaNs: (87024, 62)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5550\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-15T00:00:00.000000000 to 2024-11-27T00:00:00.000000000\n",
      "Val: 2024-11-27T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.593040  0.336168  0.429099   8466.000000\n",
      "1              0.420917  0.676549  0.518961   6038.000000\n",
      "accuracy       0.477868  0.477868  0.477868      0.477868\n",
      "macro avg      0.506979  0.506358  0.474030  14504.000000\n",
      "weighted avg   0.521386  0.477868  0.466508  14504.000000\n",
      "\n",
      "Testing with enhanced features:\n",
      "\n",
      "Running experiment with 12 features\n",
      "Shape after dropping NaNs: (73402, 62)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5557\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-08-02T00:00:00.000000000 to 2024-12-03T00:00:00.000000000\n",
      "Val: 2024-12-03T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.615096  0.500998  0.552215   7515.000000\n",
      "1              0.386453  0.500636  0.436196   4718.000000\n",
      "accuracy       0.500858  0.500858  0.500858      0.500858\n",
      "macro avg      0.500774  0.500817  0.494205  12233.000000\n",
      "weighted avg   0.526913  0.500858  0.507469  12233.000000\n",
      "\n",
      "=== Testing Deep NN ===\n",
      "\n",
      "Testing with base features:\n",
      "\n",
      "Running experiment with 5 features\n",
      "Shape after dropping NaNs: (87024, 62)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5498\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-15T00:00:00.000000000 to 2024-11-27T00:00:00.000000000\n",
      "Val: 2024-11-27T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.594384  0.370069  0.456140   8466.000000\n",
      "1              0.422398  0.645909  0.510772   6038.000000\n",
      "accuracy       0.484901  0.484901  0.484901      0.484901\n",
      "macro avg      0.508391  0.507989  0.483456  14504.000000\n",
      "weighted avg   0.522787  0.484901  0.478883  14504.000000\n",
      "\n",
      "Testing with enhanced features:\n",
      "\n",
      "Running experiment with 12 features\n",
      "Shape after dropping NaNs: (73402, 62)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5568\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-08-02T00:00:00.000000000 to 2024-12-03T00:00:00.000000000\n",
      "Val: 2024-12-03T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.630446  0.491018  0.552065   7515.000000\n",
      "1              0.400470  0.541543  0.460443   4718.000000\n",
      "accuracy       0.510504  0.510504  0.510504      0.510504\n",
      "macro avg      0.515458  0.516280  0.506254  12233.000000\n",
      "weighted avg   0.541749  0.510504  0.516728  12233.000000\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Simple NN:\n",
      "Base features AUC: 0.5550\n",
      "Enhanced features AUC: 0.5557\n",
      "\n",
      "Deep NN:\n",
      "Base features AUC: 0.5498\n",
      "Enhanced features AUC: 0.5568\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_features(df):\n",
    "    \"\"\"Create enhanced feature set from base dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Non-linear transformations of top features\n",
    "    df['skew_trend_10d_squared'] = df['skew_trend_10d'] ** 2\n",
    "    df['skew_trend_10d_cubed'] = df['skew_trend_10d'] ** 3\n",
    "    df['skew_volatility_squared'] = df['skew_volatility'] ** 2\n",
    "    \n",
    "    # 2. Exponential moving averages\n",
    "    for window in [3, 5, 10]:\n",
    "        df[f'skew_ema_{window}d'] = df.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "            lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # 3. RSI for skew\n",
    "    df['skew_rsi'] = df.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "        lambda x: (\n",
    "            100 - (100 / (1 + (\n",
    "                x.diff().clip(lower=0).rolling(window=14).mean() /\n",
    "                -x.diff().clip(upper=0).rolling(window=14).mean()\n",
    "            )))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_experiment(model_data, features, model, target='signal_5d_3pct', n_splits=5):\n",
    "    \"\"\"Run complete experiment with cross validation.\"\"\"\n",
    "    print(f\"\\nRunning experiment with {len(features)} features\")\n",
    "    \n",
    "    # Drop NaN values first\n",
    "    valid_data = model_data.dropna(subset=features + [target])\n",
    "    print(f\"Shape after dropping NaNs: {valid_data.shape}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = valid_data[features].values\n",
    "    y = valid_data[target].values\n",
    "    dates = valid_data['date'].values\n",
    "    \n",
    "    # Time series cross validation\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Evaluate each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"Training fold {fold+1}/{n_splits}...\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        \n",
    "        # Store metrics\n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_dates': (dates[train_idx].min(), dates[train_idx].max()),\n",
    "            'val_dates': (dates[val_idx].min(), dates[val_idx].max()),\n",
    "            'roc_auc': roc_auc_score(y_val, y_pred_proba),\n",
    "            'report': classification_report(y_val, y_pred, output_dict=True)\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    avg_auc = np.mean([m['roc_auc'] for m in fold_metrics])\n",
    "    print(f\"\\nAverage ROC AUC: {avg_auc:.4f}\")\n",
    "    \n",
    "    # Print last fold details\n",
    "    last_fold = fold_metrics[-1]\n",
    "    print(f\"\\nLast Fold Results:\")\n",
    "    print(f\"Train: {last_fold['train_dates'][0]} to {last_fold['train_dates'][1]}\")\n",
    "    print(f\"Val: {last_fold['val_dates'][0]} to {last_fold['val_dates'][1]}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pd.DataFrame(last_fold['report']).transpose())\n",
    "    \n",
    "    return fold_metrics\n",
    "\n",
    "# Create enhanced features\n",
    "print(\"Creating enhanced features...\")\n",
    "model_data = create_enhanced_features(model_data)\n",
    "\n",
    "# Define feature sets\n",
    "base_features = [\n",
    "    'skew_trend_10d', 'skew_volatility', 'log_moneyness_skew',\n",
    "    'log_option_volume', 'volume_spread'\n",
    "]\n",
    "\n",
    "enhanced_features = base_features + [\n",
    "    'skew_trend_10d_squared', 'skew_trend_10d_cubed', \n",
    "    'skew_volatility_squared',\n",
    "    'skew_ema_3d', 'skew_ema_5d', 'skew_ema_10d',\n",
    "    'skew_rsi'\n",
    "]\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Simple NN': MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 25),\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        learning_rate_init=0.001,\n",
    "        batch_size=256,\n",
    "        alpha=0.0005,\n",
    "        activation='tanh',\n",
    "        solver='adam'\n",
    "    ),\n",
    "    'Deep NN': MLPClassifier(\n",
    "        hidden_layer_sizes=(200, 100, 50),\n",
    "        max_iter=2000,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        learning_rate_init=0.0005,\n",
    "        batch_size=128,\n",
    "        alpha=0.0001,\n",
    "        activation='relu',\n",
    "        solver='adam'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run experiments\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== Testing {model_name} ===\")\n",
    "    \n",
    "    # Test with base features\n",
    "    print(\"\\nTesting with base features:\")\n",
    "    base_results = run_experiment(model_data, base_features, model)\n",
    "    \n",
    "    # Test with enhanced features\n",
    "    print(\"\\nTesting with enhanced features:\")\n",
    "    enhanced_results = run_experiment(model_data, enhanced_features, model)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'base': base_results,\n",
    "        'enhanced': enhanced_results\n",
    "    }\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Summary ===\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Base features AUC: {np.mean([m['roc_auc'] for m in model_results['base']]):.4f}\")\n",
    "    print(f\"Enhanced features AUC: {np.mean([m['roc_auc'] for m in model_results['enhanced']]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features...\n",
      "\n",
      "=== Testing Optimized Model ===\n",
      "\n",
      "Running experiment with 11 features\n",
      "Shape after dropping NaNs: (73402, 68)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5544\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-08-02T00:00:00.000000000 to 2024-12-03T00:00:00.000000000\n",
      "Val: 2024-12-03T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.634567  0.435795  0.516725   7515.000000\n",
      "1              0.400452  0.600254  0.480407   4718.000000\n",
      "accuracy       0.499223  0.499223  0.499223      0.499223\n",
      "macro avg      0.517510  0.518025  0.498566  12233.000000\n",
      "weighted avg   0.544274  0.499223  0.502718  12233.000000\n",
      "\n",
      "=== Final Results ===\n",
      "Optimized Model AUC: 0.5544\n"
     ]
    }
   ],
   "source": [
    "# 1. Add more sophisticated features\n",
    "def create_advanced_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Volatility-adjusted skew metrics\n",
    "    df['vol_adj_skew'] = df['log_moneyness_skew'] / df['rolling_vol']\n",
    "    df['vol_adj_skew_trend'] = df['skew_trend_10d'] / df['rolling_vol']\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df['skew_momentum_1d'] = df.groupby('underlying_ticker')['log_moneyness_skew'].diff()\n",
    "    df['skew_momentum_5d'] = df.groupby('underlying_ticker')['log_moneyness_skew'].diff(5)\n",
    "    \n",
    "    # Moving average crossovers\n",
    "    df['skew_ma_cross'] = df['skew_ema_3d'] - df['skew_ema_10d']\n",
    "    df['skew_ma_cross_signal'] = np.sign(df['skew_ma_cross'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Define optimized feature set\n",
    "optimized_features = [\n",
    "    # Core features (best performers)\n",
    "    'skew_trend_10d', 'skew_volatility', \n",
    "    'vol_adj_skew', 'vol_adj_skew_trend',\n",
    "    \n",
    "    # Momentum features\n",
    "    'skew_momentum_1d', 'skew_momentum_5d',\n",
    "    'skew_ma_cross', 'skew_ma_cross_signal',\n",
    "    \n",
    "    # Technical indicators\n",
    "    'skew_rsi', 'skew_ema_3d', 'skew_ema_10d'\n",
    "]\n",
    "\n",
    "# 3. Optimized model architecture\n",
    "optimized_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50, 25),  # Balanced architecture\n",
    "    max_iter=1500,                     # More iterations\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    learning_rate_init=0.0008,         # Balanced learning rate\n",
    "    batch_size=192,                    # Balanced batch size\n",
    "    alpha=0.0003,                      # Balanced regularization\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    validation_fraction=0.2,           # Larger validation set\n",
    "    n_iter_no_change=15               # More patience\n",
    ")\n",
    "\n",
    "# Run new experiment\n",
    "print(\"Creating advanced features...\")\n",
    "model_data = create_advanced_features(model_data)\n",
    "\n",
    "print(\"\\n=== Testing Optimized Model ===\")\n",
    "optimized_results = run_experiment(model_data, optimized_features, optimized_nn)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"Optimized Model AUC: {np.mean([m['roc_auc'] for m in optimized_results]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing GBM ===\n",
      "\n",
      "Running experiment with 12 features\n",
      "Shape after dropping NaNs: (73402, 68)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5551\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-08-02T00:00:00.000000000 to 2024-12-03T00:00:00.000000000\n",
      "Val: 2024-12-03T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.632269  0.429674  0.511646   7515.000000\n",
      "1              0.398541  0.601950  0.479568   4718.000000\n",
      "accuracy       0.496117  0.496117  0.496117      0.496117\n",
      "macro avg      0.515405  0.515812  0.495607  12233.000000\n",
      "weighted avg   0.542125  0.496117  0.499274  12233.000000\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                   feature  importance\n",
      "8             skew_ema_10d    0.349865\n",
      "1          skew_volatility    0.125550\n",
      "6            volume_spread    0.092585\n",
      "5        log_option_volume    0.073199\n",
      "11           skew_ma_cross    0.059611\n",
      "9                 skew_rsi    0.059597\n",
      "4             vol_adj_skew    0.053344\n",
      "0           skew_trend_10d    0.049815\n",
      "3   skew_trend_10d_squared    0.038250\n",
      "10        skew_momentum_5d    0.037841\n",
      "\n",
      "=== Testing RF ===\n",
      "\n",
      "Running experiment with 12 features\n",
      "Shape after dropping NaNs: (73402, 68)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5526\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-08-02T00:00:00.000000000 to 2024-12-03T00:00:00.000000000\n",
      "Val: 2024-12-03T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.618862  0.400798  0.486513   7515.000000\n",
      "1              0.388678  0.606825  0.473850   4718.000000\n",
      "accuracy       0.480258  0.480258  0.480258      0.480258\n",
      "macro avg      0.503770  0.503812  0.480181  12233.000000\n",
      "weighted avg   0.530085  0.480258  0.481629  12233.000000\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                   feature  importance\n",
      "8             skew_ema_10d    0.237996\n",
      "0           skew_trend_10d    0.192183\n",
      "7              skew_ema_3d    0.106175\n",
      "1          skew_volatility    0.081180\n",
      "6            volume_spread    0.069104\n",
      "4             vol_adj_skew    0.063827\n",
      "5        log_option_volume    0.055346\n",
      "3   skew_trend_10d_squared    0.048997\n",
      "11           skew_ma_cross    0.048238\n",
      "2       log_moneyness_skew    0.041224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Define core features that worked best\n",
    "core_features = [\n",
    "    # Strongest predictors from previous runs\n",
    "    'skew_trend_10d', 'skew_volatility', 'log_moneyness_skew',\n",
    "    'skew_trend_10d_squared', 'vol_adj_skew',\n",
    "    \n",
    "    # Volume-based features\n",
    "    'log_option_volume', 'volume_spread',\n",
    "    \n",
    "    # Technical indicators\n",
    "    'skew_ema_3d', 'skew_ema_10d', 'skew_rsi',\n",
    "    \n",
    "    # Momentum features\n",
    "    'skew_momentum_5d', 'skew_ma_cross'\n",
    "]\n",
    "\n",
    "# Define ensemble models\n",
    "models = {\n",
    "    'GBM': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=50,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'RF': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=50,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run experiments with ensemble models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== Testing {model_name} ===\")\n",
    "    results = run_experiment(model_data, core_features, model)\n",
    "    \n",
    "    # Print feature importances\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': core_features,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Feature Importances:\")\n",
    "        print(importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating focused features...\n",
      "\n",
      "=== Testing Focused GBM ===\n",
      "\n",
      "Running experiment with 11 features\n",
      "Shape after dropping NaNs: (82147, 75)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "\n",
      "Average ROC AUC: 0.5579\n",
      "\n",
      "Last Fold Results:\n",
      "Train: 2024-07-22T00:00:00.000000000 to 2024-11-29T00:00:00.000000000\n",
      "Val: 2024-11-29T00:00:00.000000000 to 2024-12-24T00:00:00.000000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.615313  0.405525  0.488863   8145.000000\n",
      "1              0.418239  0.627660  0.501983   5546.000000\n",
      "accuracy       0.495508  0.495508  0.495508      0.495508\n",
      "macro avg      0.516776  0.516592  0.495423  13691.000000\n",
      "weighted avg   0.535481  0.495508  0.494177  13691.000000\n",
      "\n",
      "Feature Importances:\n",
      "                feature  importance\n",
      "0          skew_ema_10d    0.302049\n",
      "1       skew_volatility    0.111252\n",
      "9          volume_trend    0.100343\n",
      "2         volume_spread    0.093511\n",
      "10      spread_momentum    0.078447\n",
      "4          vol_adj_skew    0.065077\n",
      "8   vol_weighted_spread    0.061543\n",
      "5             ema_ratio    0.051646\n",
      "7             ema_trend    0.048919\n",
      "6              ema_diff    0.044392\n",
      "3           skew_ema_3d    0.042819\n"
     ]
    }
   ],
   "source": [
    "def create_focused_features(df):\n",
    "    \"\"\"Create enhanced feature set with infinity handling.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. EMA-based features\n",
    "    df['skew_ema_3d'] = df.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "        lambda x: x.ewm(span=3, adjust=False).mean()\n",
    "    )\n",
    "    df['skew_ema_10d'] = df.groupby('underlying_ticker')['log_moneyness_skew'].transform(\n",
    "        lambda x: x.ewm(span=10, adjust=False).mean()\n",
    "    )\n",
    "    \n",
    "    # Safe division function\n",
    "    def safe_divide(a, b, fill_value=0):\n",
    "        result = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "        return np.clip(result, -1e6, 1e6)  # Clip extreme values\n",
    "    \n",
    "    # 2. Ratio and difference features (with safe operations)\n",
    "    df['ema_ratio'] = safe_divide(df['skew_ema_3d'], df['skew_ema_10d'])\n",
    "    df['ema_diff'] = df['skew_ema_3d'] - df['skew_ema_10d']\n",
    "    \n",
    "    # 3. Trend features (with safe operations)\n",
    "    df['ema_trend'] = df.groupby('underlying_ticker')['skew_ema_10d'].transform(\n",
    "        lambda x: x.pct_change(5).clip(-10, 10)  # Clip extreme changes\n",
    "    )\n",
    "    \n",
    "    # 4. Volatility interactions\n",
    "    df['vol_ema_ratio'] = df['skew_volatility'] * df['ema_ratio']\n",
    "    df['vol_weighted_spread'] = df['skew_volatility'] * df['volume_spread']\n",
    "    \n",
    "    # 5. Volume-based signals (with clipping)\n",
    "    df['volume_trend'] = df.groupby('underlying_ticker')['log_option_volume'].transform(\n",
    "        lambda x: x.pct_change(5).clip(-10, 10)\n",
    "    )\n",
    "    df['spread_momentum'] = df.groupby('underlying_ticker')['volume_spread'].transform(\n",
    "        lambda x: x.pct_change(3).clip(-10, 10)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define focused feature set\n",
    "focused_features = [\n",
    "    # Core features\n",
    "    'skew_ema_10d', 'skew_volatility', 'volume_spread',\n",
    "    'skew_ema_3d', 'vol_adj_skew',\n",
    "    \n",
    "    # Derived features\n",
    "    'ema_ratio', 'ema_diff', 'ema_trend',\n",
    "    'vol_weighted_spread', 'volume_trend', 'spread_momentum'\n",
    "]\n",
    "\n",
    "# Create optimized GBM\n",
    "focused_gbm = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=30,\n",
    "    subsample=0.85,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run experiment\n",
    "print(\"Creating focused features...\")\n",
    "model_data = create_focused_features(model_data)\n",
    "\n",
    "print(\"\\n=== Testing Focused GBM ===\")\n",
    "focused_results = run_experiment(model_data, focused_features, focused_gbm)\n",
    "\n",
    "# Print feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': focused_features,\n",
    "    'importance': focused_gbm.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../data/processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the clean dataframe as CSV\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed/clean_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclean_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Financial Data/.venv/lib/python3.13/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../data/processed'"
     ]
    }
   ],
   "source": [
    "# Save the clean dataframe as CSV\n",
    "output_path = '../data/processed/clean_df.csv'\n",
    "clean_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset shape: {clean_df.shape}\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Display first few rows and data info\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(clean_df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(clean_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (125000, 33)\n",
      "Saved to: ./data/processed/clean_df.csv\n",
      "\n",
      "First few rows:\n",
      "           date underlying_ticker       open       high        low      close  \\\n",
      "0    2024-07-01                AA  40.021413  40.637890  39.315446  39.474537   \n",
      "1000 2024-07-02                AA  39.663458  40.916304  39.627663  40.598122   \n",
      "2000 2024-07-03                AA  41.353798  42.755791  41.343857  42.139313   \n",
      "3000 2024-07-05                AA  42.407781  42.855222  42.179084  42.198971   \n",
      "4000 2024-07-08                AA  42.298404  42.308346  41.075390  41.423401   \n",
      "\n",
      "         volume  total_option_volume  option_hhi  volume_weighted_hhi  ...  \\\n",
      "0     3228000.0              18243.0    0.044966             0.000033  ...   \n",
      "1000  4039500.0              35060.0    0.172495             0.000252  ...   \n",
      "2000  4074500.0              32198.0    0.071051             0.000104  ...   \n",
      "3000  2670200.0              14792.0    0.033550             0.000016  ...   \n",
      "4000  3001000.0              12452.0    0.032215             0.000016  ...   \n",
      "\n",
      "      excess_ret_20  signal_5d_2pct  signal_10d_2pct  signal_20d_2pct  \\\n",
      "0         -0.193624               0                1                1   \n",
      "1000      -0.194070               1                1                1   \n",
      "2000      -0.242110               1                1                1   \n",
      "3000      -0.266411               1                1                1   \n",
      "4000      -0.249077               1                1                1   \n",
      "\n",
      "      signal_5d_3pct  signal_10d_3pct  signal_20d_3pct  signal_5d_5pct  \\\n",
      "0                  0                1                1               0   \n",
      "1000               1                1                1               1   \n",
      "2000               1                1                1               1   \n",
      "3000               1                1                1               1   \n",
      "4000               1                1                1               1   \n",
      "\n",
      "      signal_10d_5pct  signal_20d_5pct  \n",
      "0                   1                1  \n",
      "1000                1                1  \n",
      "2000                1                1  \n",
      "3000                1                1  \n",
      "4000                1                1  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 125000 entries, 0 to 124999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                    Non-Null Count   Dtype         \n",
      "---  ------                    --------------   -----         \n",
      " 0   date                      125000 non-null  datetime64[ns]\n",
      " 1   underlying_ticker         125000 non-null  object        \n",
      " 2   open                      121780 non-null  float64       \n",
      " 3   high                      121780 non-null  float64       \n",
      " 4   low                       121780 non-null  float64       \n",
      " 5   close                     121780 non-null  float64       \n",
      " 6   volume                    121780 non-null  float64       \n",
      " 7   total_option_volume       125000 non-null  float64       \n",
      " 8   option_hhi                125000 non-null  float64       \n",
      " 9   volume_weighted_hhi       124000 non-null  float64       \n",
      " 10  weighted_strike_distance  125000 non-null  float64       \n",
      " 11  log_moneyness_skew        125000 non-null  float64       \n",
      " 12  weighted_mean_dte         125000 non-null  float64       \n",
      " 13  weighted_var_dte          125000 non-null  float64       \n",
      " 14  weighted_std_dte          125000 non-null  float64       \n",
      " 15  os_ratio                  125000 non-null  float64       \n",
      " 16  os_ratio_ma20             121000 non-null  float64       \n",
      " 17  normalized_os_ratio       125000 non-null  float64       \n",
      " 18  forward_ret_5             116900 non-null  float64       \n",
      " 19  excess_ret_5              116900 non-null  float64       \n",
      " 20  forward_ret_10            112020 non-null  float64       \n",
      " 21  excess_ret_10             112020 non-null  float64       \n",
      " 22  forward_ret_20            102260 non-null  float64       \n",
      " 23  excess_ret_20             102260 non-null  float64       \n",
      " 24  signal_5d_2pct            125000 non-null  int64         \n",
      " 25  signal_10d_2pct           125000 non-null  int64         \n",
      " 26  signal_20d_2pct           125000 non-null  int64         \n",
      " 27  signal_5d_3pct            125000 non-null  int64         \n",
      " 28  signal_10d_3pct           125000 non-null  int64         \n",
      " 29  signal_20d_3pct           125000 non-null  int64         \n",
      " 30  signal_5d_5pct            125000 non-null  int64         \n",
      " 31  signal_10d_5pct           125000 non-null  int64         \n",
      " 32  signal_20d_5pct           125000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(22), int64(9), object(1)\n",
      "memory usage: 32.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save the clean dataframe as CSV\n",
    "output_path = './data/processed/clean_df.csv'\n",
    "clean_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset shape: {clean_df.shape}\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Display first few rows and data info\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(clean_df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(clean_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
